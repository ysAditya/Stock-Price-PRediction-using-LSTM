{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxPa4ZL6_FKY"
      },
      "outputs": [],
      "source": [
        "import pandas_datareader as DataReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KfdHcBC_i24",
        "outputId": "358fa2f1-b14b-4908-a170-7f88f4eabb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.9/dist-packages (0.2.14)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.3.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from yfinance) (40.0.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xpxkg4j_UPu"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBpdSXRC_WiC",
        "outputId": "aa94eddd-ee7b-453f-d00f-faab31f38aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "ef = yf.download('INFY.NS','2013-04-01','2023-03-31')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "023wtZVN_x4q"
      },
      "outputs": [],
      "source": [
        "ef_close = ef['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCBZ4Low_1UF",
        "outputId": "4a533cb2-d9c4-4b58-9080-2d5b001afd78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2013-04-01    368.024994\n",
              "2013-04-02    371.131256\n",
              "2013-04-03    371.056244\n",
              "2013-04-04    360.006256\n",
              "2013-04-05    358.231262\n",
              "Name: Close, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "ef_close.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh2Lv8Zr_5oi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "MGmljTwzAFc2",
        "outputId": "1e30dab1-8474-4dc0-a276-8ed37bd8c6f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(ef_close)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1MQzrDCAMU0"
      },
      "outputs": [],
      "source": [
        "ef_close = np.array(ef_close)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYC2AeCmFQh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07UO3CgAAs25"
      },
      "outputs": [],
      "source": [
        "seq_length = 10\n",
        "pred_seq = 5\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for idx in range(int(2467/pred_seq) - 2):\n",
        "   X.append(ef_close[idx*pred_seq: idx*pred_seq + seq_length])\n",
        "   Y.append(ef_close[idx*pred_seq + seq_length: idx*pred_seq + seq_length + pred_seq])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rame5Ct6DDky",
        "outputId": "67d25524-8b61-4974-c520-540ac5e35aa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((491, 10), (491, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "X.shape, Y.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "JMDh3D44FNRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, x_test),(y_train, y_test) = ((X[:400,:], X[400:,:]), (Y[:400,:], Y[400:,:]))"
      ],
      "metadata": {
        "id": "cR62iFv5HBQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDk_SZDnHBhH",
        "outputId": "bb6711f8-dc6f-41cd-d393-58e69ce22cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(TensorDataset(torch.tensor(x_train).reshape(-1,10,1).float(),\\\n",
        "                                        torch.tensor(y_train).reshape(-1,5).float()),\n",
        "                          batch_size = 32, shuffle = True)\n",
        "\n",
        "x_test, y_test = (torch.tensor(x_test).reshape(-1,10,1).float(),\n",
        "                  torch.tensor(y_test).reshape(-1,5).float())"
      ],
      "metadata": {
        "id": "Si_KBMWCHHSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhUk5EZCHI_1",
        "outputId": "fe079194-8b0b-45f2-96b3-f48633af0b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMnet, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_size = 1, hidden_size = 128, batch_first = True,\n",
        "                        num_layers = 2, dropout = 0.3)\n",
        "    self.bn1 = nn.BatchNorm1d(num_features = 128)\n",
        "    self.fc1 = nn.Linear(in_features = 128, out_features = 64)\n",
        "    self.fc2 = nn.Linear(in_features = 64, out_features = 5)\n",
        "\n",
        "  # Return array/weights initialized to 0 matching hidden size\n",
        "  def initialize(self, batch_size):\n",
        "    return(torch.zeros(2, batch_size, 128).to(device),\n",
        "           torch.zeros(2, batch_size, 128).to(device))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x, s = self.lstm(x, self.initialize(x.size()[0]))\n",
        "    # print(x.size())\n",
        "    x = x[:, -1, :] # won't see in MLP, DNN, CNN.\n",
        "                    # [Batch, outcome of last cell, something batch normalisation]\n",
        "                    # Only interested in the last one not intermediate ones.\n",
        "                    # This is seen from the -1 argument passed to x[]\n",
        "    x = self.bn1(x)\n",
        "\n",
        "    x = F.relu(self.fc1(x)) # Don't have negative values for price\n",
        "                            # Therefore Relu good candidate than tanh or sigmoid\n",
        "                            # Ultimately dealing with regression problem\n",
        "                            # For profit, can have negative or positive\n",
        "                            # Leaky Relu preferable\n",
        "                            # For standardised profit, tanh can be good candidate.\n",
        "    x = self.fc2(x)\n",
        "    # print(x.size())\n",
        "    return x"
      ],
      "metadata": {
        "id": "iy-knQZUH3kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMnet()\n",
        "opt = Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "BZGEMyKZMfmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary as summary"
      ],
      "metadata": {
        "id": "r5CqIDzxSZ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size = (32, 10, 1), device = 'cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkwwtm4-SXSr",
        "outputId": "7a9c23ee-664e-482d-aa4e-82103cf31a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "LSTMnet                                  [32, 5]                   --\n",
              "LSTM: 1-1                              [32, 10, 128]             199,168\n",
              "BatchNorm1d: 1-2                       [32, 128]                 256\n",
              "Linear: 1-3                            [32, 64]                  8,256\n",
              "Linear: 1-4                            [32, 5]                   325\n",
              "==========================================================================================\n",
              "Total params: 208,005\n",
              "Trainable params: 208,005\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 64.02\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.38\n",
              "Params size (MB): 0.83\n",
              "Estimated Total Size (MB): 1.21\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.0\n",
        "  val_loss = 0.0\n",
        "  \n",
        "  for batch, target in train_loader:\n",
        "    batch = batch.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    output = model(batch)\n",
        "    loss = F.mse_loss(output, target)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    train_loss = train_loss + loss.item()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x_test = x_test.to(device)\n",
        "      y_test = y_test.to(device)\n",
        "      val_output = model(x_test)\n",
        "      loss_val = F.mse_loss(val_output, y_test)\n",
        "      val_loss = val_loss + loss_val.item()\n",
        "\n",
        "      if(epoch+1)%10==0:\n",
        "        print(f\"Train loss: {train_loss} and Val loss: {val_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHKjxDblMqTo",
        "outputId": "5b68409e-cb91-438a-a14c-26797b9f423c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 291707.96875 and Val loss: 2505448.25\n",
            "Train loss: 711620.46875 and Val loss: 5005081.0\n",
            "Train loss: 1003451.21875 and Val loss: 7502064.0\n",
            "Train loss: 1350837.3125 and Val loss: 10001895.5\n",
            "Train loss: 1591058.234375 and Val loss: 12483591.0\n",
            "Train loss: 1878698.484375 and Val loss: 14952603.25\n",
            "Train loss: 2207440.640625 and Val loss: 17400949.0\n",
            "Train loss: 2436559.34375 and Val loss: 19862870.75\n",
            "Train loss: 2675091.140625 and Val loss: 22340435.5\n",
            "Train loss: 2982768.296875 and Val loss: 24806076.25\n",
            "Train loss: 3280844.265625 and Val loss: 27283516.75\n",
            "Train loss: 3478223.640625 and Val loss: 29752048.25\n",
            "Train loss: 3780372.953125 and Val loss: 32221903.5\n",
            "Train loss: 87561.8046875 and Val loss: 1980415.75\n",
            "Train loss: 143648.14453125 and Val loss: 3989568.875\n",
            "Train loss: 255687.28515625 and Val loss: 5988381.5\n",
            "Train loss: 327591.12109375 and Val loss: 8031689.0\n",
            "Train loss: 490333.07421875 and Val loss: 10082075.5\n",
            "Train loss: 592069.78515625 and Val loss: 12123484.25\n",
            "Train loss: 683103.36328125 and Val loss: 14147443.125\n",
            "Train loss: 763870.85546875 and Val loss: 16166779.375\n",
            "Train loss: 855092.26171875 and Val loss: 18202434.875\n",
            "Train loss: 936535.53515625 and Val loss: 20238871.125\n",
            "Train loss: 1018685.94140625 and Val loss: 22225838.125\n",
            "Train loss: 1079138.60546875 and Val loss: 24156553.875\n",
            "Train loss: 1127048.92578125 and Val loss: 26034771.5\n",
            "Train loss: 114036.203125 and Val loss: 1795448.25\n",
            "Train loss: 171738.38671875 and Val loss: 3586608.5\n",
            "Train loss: 231730.13671875 and Val loss: 5382894.25\n",
            "Train loss: 283026.9140625 and Val loss: 7177263.875\n",
            "Train loss: 340238.75390625 and Val loss: 8965760.875\n",
            "Train loss: 476676.62890625 and Val loss: 10746893.375\n",
            "Train loss: 527479.125 and Val loss: 12531983.75\n",
            "Train loss: 584183.3125 and Val loss: 14291179.25\n",
            "Train loss: 710858.6015625 and Val loss: 16056563.5\n",
            "Train loss: 763615.43359375 and Val loss: 17827583.375\n",
            "Train loss: 805095.171875 and Val loss: 19585996.5\n",
            "Train loss: 860280.296875 and Val loss: 21336510.0\n",
            "Train loss: 930534.0234375 and Val loss: 23078448.875\n",
            "Train loss: 18027.494140625 and Val loss: 1514537.375\n",
            "Train loss: 35841.865234375 and Val loss: 3031375.625\n",
            "Train loss: 69697.583984375 and Val loss: 4544096.375\n",
            "Train loss: 73396.84692382812 and Val loss: 6059146.875\n",
            "Train loss: 83040.70825195312 and Val loss: 7572281.25\n",
            "Train loss: 157999.70043945312 and Val loss: 9098356.375\n",
            "Train loss: 168972.49145507812 and Val loss: 10623490.125\n",
            "Train loss: 209791.19067382812 and Val loss: 12158648.125\n",
            "Train loss: 246960.64379882812 and Val loss: 13708192.25\n",
            "Train loss: 272160.8781738281 and Val loss: 15233229.875\n",
            "Train loss: 330406.4914550781 and Val loss: 16758709.25\n",
            "Train loss: 343915.1789550781 and Val loss: 18278522.5\n",
            "Train loss: 361830.9836425781 and Val loss: 19792244.625\n",
            "Train loss: 17507.599609375 and Val loss: 1358647.75\n",
            "Train loss: 46282.73828125 and Val loss: 2715703.125\n",
            "Train loss: 55130.6123046875 and Val loss: 4073463.625\n",
            "Train loss: 76632.6259765625 and Val loss: 5431633.75\n",
            "Train loss: 89538.068359375 and Val loss: 6790212.25\n",
            "Train loss: 98477.7783203125 and Val loss: 8144543.75\n",
            "Train loss: 102130.55346679688 and Val loss: 9506638.0\n",
            "Train loss: 123673.20776367188 and Val loss: 10858257.0\n",
            "Train loss: 127056.138671875 and Val loss: 12213342.75\n",
            "Train loss: 136232.0 and Val loss: 13567922.875\n",
            "Train loss: 145178.4765625 and Val loss: 14921133.0\n",
            "Train loss: 156865.03515625 and Val loss: 16276718.875\n",
            "Train loss: 163590.0283203125 and Val loss: 17633198.25\n",
            "Train loss: 9394.703125 and Val loss: 1250097.875\n",
            "Train loss: 26322.251953125 and Val loss: 2496524.0\n",
            "Train loss: 29121.96044921875 and Val loss: 3738116.625\n",
            "Train loss: 40723.08544921875 and Val loss: 4977599.375\n",
            "Train loss: 44243.827392578125 and Val loss: 6213915.25\n",
            "Train loss: 49682.596923828125 and Val loss: 7448402.5\n",
            "Train loss: 63823.343994140625 and Val loss: 8678550.25\n",
            "Train loss: 66938.57421875 and Val loss: 9908982.5\n",
            "Train loss: 74595.45849609375 and Val loss: 11137879.5\n",
            "Train loss: 90681.73974609375 and Val loss: 12369831.625\n",
            "Train loss: 101257.91357421875 and Val loss: 13603971.125\n",
            "Train loss: 106421.568359375 and Val loss: 14839965.875\n",
            "Train loss: 135159.05859375 and Val loss: 16073590.25\n",
            "Train loss: 2402.929931640625 and Val loss: 1187595.625\n",
            "Train loss: 8075.454345703125 and Val loss: 2371719.0\n",
            "Train loss: 12677.505126953125 and Val loss: 3555369.625\n",
            "Train loss: 23969.766845703125 and Val loss: 4736902.25\n",
            "Train loss: 32594.316650390625 and Val loss: 5918622.375\n",
            "Train loss: 37374.983642578125 and Val loss: 7101458.0\n",
            "Train loss: 39417.343017578125 and Val loss: 8280606.625\n",
            "Train loss: 44593.170166015625 and Val loss: 9460963.625\n",
            "Train loss: 46379.28723144531 and Val loss: 10641232.0\n",
            "Train loss: 47667.69104003906 and Val loss: 11824287.375\n",
            "Train loss: 50352.74255371094 and Val loss: 13009066.0\n",
            "Train loss: 56321.59411621094 and Val loss: 14194936.5\n",
            "Train loss: 72552.08532714844 and Val loss: 15382537.75\n",
            "Train loss: 8167.2001953125 and Val loss: 1151815.25\n",
            "Train loss: 12562.73095703125 and Val loss: 2303607.625\n",
            "Train loss: 16023.79736328125 and Val loss: 3450003.25\n",
            "Train loss: 19089.860107421875 and Val loss: 4597335.375\n",
            "Train loss: 20555.310913085938 and Val loss: 5740653.875\n",
            "Train loss: 24304.983520507812 and Val loss: 6884358.0\n",
            "Train loss: 28126.535034179688 and Val loss: 8028152.0\n",
            "Train loss: 33254.78649902344 and Val loss: 9169509.5\n",
            "Train loss: 37604.57946777344 and Val loss: 10312404.875\n",
            "Train loss: 45773.42175292969 and Val loss: 11455395.125\n",
            "Train loss: 51283.23767089844 and Val loss: 12598517.5\n",
            "Train loss: 55560.93395996094 and Val loss: 13741325.125\n",
            "Train loss: 61287.39294433594 and Val loss: 14889642.75\n",
            "Train loss: 1605.4527587890625 and Val loss: 1094728.875\n",
            "Train loss: 5163.7340087890625 and Val loss: 2188277.625\n",
            "Train loss: 9904.290161132812 and Val loss: 3282039.375\n",
            "Train loss: 15699.031860351562 and Val loss: 4379605.0\n",
            "Train loss: 22758.818481445312 and Val loss: 5472716.5\n",
            "Train loss: 28848.138305664062 and Val loss: 6569206.75\n",
            "Train loss: 33674.63000488281 and Val loss: 7663748.5\n",
            "Train loss: 37982.98937988281 and Val loss: 8756805.125\n",
            "Train loss: 43617.11437988281 and Val loss: 9852545.875\n",
            "Train loss: 49526.98547363281 and Val loss: 10949236.375\n",
            "Train loss: 52310.02307128906 and Val loss: 12047021.625\n",
            "Train loss: 61451.06311035156 and Val loss: 13143990.125\n",
            "Train loss: 66271.72863769531 and Val loss: 14240819.25\n",
            "Train loss: 3080.569091796875 and Val loss: 1052684.875\n",
            "Train loss: 4892.660888671875 and Val loss: 2103582.25\n",
            "Train loss: 8314.33154296875 and Val loss: 3155217.125\n",
            "Train loss: 16981.20947265625 and Val loss: 4208360.625\n",
            "Train loss: 23102.11279296875 and Val loss: 5257816.625\n",
            "Train loss: 35009.73095703125 and Val loss: 6308103.0\n",
            "Train loss: 37246.5361328125 and Val loss: 7358171.875\n",
            "Train loss: 39342.84326171875 and Val loss: 8407502.25\n",
            "Train loss: 43481.2216796875 and Val loss: 9453504.8125\n",
            "Train loss: 47774.45751953125 and Val loss: 10497353.75\n",
            "Train loss: 51327.39697265625 and Val loss: 11545499.0\n",
            "Train loss: 55159.510986328125 and Val loss: 12591482.0\n",
            "Train loss: 62103.388427734375 and Val loss: 13637252.9375\n",
            "Train loss: 1849.2119140625 and Val loss: 1036638.8125\n",
            "Train loss: 14542.33203125 and Val loss: 2071724.4375\n",
            "Train loss: 17136.800048828125 and Val loss: 3106568.1875\n",
            "Train loss: 20236.551025390625 and Val loss: 4136219.6875\n",
            "Train loss: 21932.246215820312 and Val loss: 5166062.1875\n",
            "Train loss: 23365.92578125 and Val loss: 6196043.125\n",
            "Train loss: 30743.98828125 and Val loss: 7227359.625\n",
            "Train loss: 34730.31103515625 and Val loss: 8256228.75\n",
            "Train loss: 36224.798828125 and Val loss: 9283142.0\n",
            "Train loss: 39060.333740234375 and Val loss: 10310941.25\n",
            "Train loss: 45532.228759765625 and Val loss: 11337571.5\n",
            "Train loss: 47981.107666015625 and Val loss: 12365686.9375\n",
            "Train loss: 50391.16552734375 and Val loss: 13397254.625\n",
            "Train loss: 2341.918212890625 and Val loss: 1027158.25\n",
            "Train loss: 4693.478515625 and Val loss: 2059649.0625\n",
            "Train loss: 9423.14599609375 and Val loss: 3088714.3125\n",
            "Train loss: 13196.258544921875 and Val loss: 4117001.125\n",
            "Train loss: 13749.443115234375 and Val loss: 5141918.75\n",
            "Train loss: 15270.283325195312 and Val loss: 6166160.375\n",
            "Train loss: 17618.447387695312 and Val loss: 7191299.8125\n",
            "Train loss: 19624.690551757812 and Val loss: 8216215.0\n",
            "Train loss: 22670.517700195312 and Val loss: 9241157.875\n",
            "Train loss: 24521.439208984375 and Val loss: 10269579.75\n",
            "Train loss: 26034.730590820312 and Val loss: 11296465.4375\n",
            "Train loss: 45107.94934082031 and Val loss: 12322461.25\n",
            "Train loss: 54376.80090332031 and Val loss: 13349026.25\n",
            "Train loss: 1357.2735595703125 and Val loss: 1027334.25\n",
            "Train loss: 3954.6939697265625 and Val loss: 2053116.25\n",
            "Train loss: 8892.419555664062 and Val loss: 3078526.8125\n",
            "Train loss: 10684.319946289062 and Val loss: 4106971.0\n",
            "Train loss: 13010.333374023438 and Val loss: 5133265.625\n",
            "Train loss: 17479.808471679688 and Val loss: 6160834.5625\n",
            "Train loss: 24232.690795898438 and Val loss: 7189198.125\n",
            "Train loss: 35672.65759277344 and Val loss: 8221730.1875\n",
            "Train loss: 40651.69860839844 and Val loss: 9253160.625\n",
            "Train loss: 42744.38220214844 and Val loss: 10284240.3125\n",
            "Train loss: 49295.26843261719 and Val loss: 11314401.5\n",
            "Train loss: 58963.35925292969 and Val loss: 12347288.0625\n",
            "Train loss: 63227.18737792969 and Val loss: 13379333.625\n",
            "Train loss: 1679.990478515625 and Val loss: 1023632.5625\n",
            "Train loss: 3471.015869140625 and Val loss: 2045644.4375\n",
            "Train loss: 4606.6265869140625 and Val loss: 3069788.6875\n",
            "Train loss: 7404.2137451171875 and Val loss: 4094307.25\n",
            "Train loss: 11018.067749023438 and Val loss: 5117909.625\n",
            "Train loss: 12168.6474609375 and Val loss: 6144113.375\n",
            "Train loss: 15578.856201171875 and Val loss: 7169184.1875\n",
            "Train loss: 20528.980224609375 and Val loss: 8191881.5\n",
            "Train loss: 23464.50732421875 and Val loss: 9216060.625\n",
            "Train loss: 35881.73388671875 and Val loss: 10236609.5\n",
            "Train loss: 40902.43701171875 and Val loss: 11259544.75\n",
            "Train loss: 43492.962158203125 and Val loss: 12276179.1875\n",
            "Train loss: 45598.1083984375 and Val loss: 13293865.0625\n",
            "Train loss: 1517.896240234375 and Val loss: 1017167.8125\n",
            "Train loss: 6116.182373046875 and Val loss: 2033498.6875\n",
            "Train loss: 7756.4429931640625 and Val loss: 3050773.8125\n",
            "Train loss: 10325.605834960938 and Val loss: 4069847.5625\n",
            "Train loss: 11009.608032226562 and Val loss: 5084570.4375\n",
            "Train loss: 12718.26025390625 and Val loss: 6099899.8125\n",
            "Train loss: 14217.211303710938 and Val loss: 7113236.5625\n",
            "Train loss: 16177.3720703125 and Val loss: 8126483.5625\n",
            "Train loss: 18092.790893554688 and Val loss: 9139249.125\n",
            "Train loss: 23178.068237304688 and Val loss: 10149483.0\n",
            "Train loss: 25766.659790039062 and Val loss: 11160473.3125\n",
            "Train loss: 27183.259643554688 and Val loss: 12172641.375\n",
            "Train loss: 28988.935302734375 and Val loss: 13183875.4375\n",
            "Train loss: 2151.562744140625 and Val loss: 1014258.25\n",
            "Train loss: 4773.826416015625 and Val loss: 2028544.8125\n",
            "Train loss: 7577.5673828125 and Val loss: 3046558.0\n",
            "Train loss: 11883.271484375 and Val loss: 4063036.4375\n",
            "Train loss: 14557.66455078125 and Val loss: 5082126.4375\n",
            "Train loss: 18795.95166015625 and Val loss: 6099580.0625\n",
            "Train loss: 19579.75518798828 and Val loss: 7117786.0625\n",
            "Train loss: 21641.223693847656 and Val loss: 8134861.5625\n",
            "Train loss: 24945.434631347656 and Val loss: 9151122.1875\n",
            "Train loss: 31598.806701660156 and Val loss: 10166010.25\n",
            "Train loss: 32665.817443847656 and Val loss: 11179576.3125\n",
            "Train loss: 33282.888916015625 and Val loss: 12195235.5\n",
            "Train loss: 34590.24182128906 and Val loss: 13212063.0\n",
            "Train loss: 1337.0308837890625 and Val loss: 1019359.5625\n",
            "Train loss: 2542.0400390625 and Val loss: 2037150.0625\n",
            "Train loss: 4052.0955810546875 and Val loss: 3052990.3125\n",
            "Train loss: 5487.696533203125 and Val loss: 4069483.5\n",
            "Train loss: 7101.5338134765625 and Val loss: 5085969.9375\n",
            "Train loss: 16449.456665039062 and Val loss: 6104141.3125\n",
            "Train loss: 17340.583435058594 and Val loss: 7125211.4375\n",
            "Train loss: 18554.359313964844 and Val loss: 8147465.875\n",
            "Train loss: 21532.26239013672 and Val loss: 9169160.5\n",
            "Train loss: 29383.96893310547 and Val loss: 10190476.5\n",
            "Train loss: 30093.853393554688 and Val loss: 11212320.3125\n",
            "Train loss: 38539.45690917969 and Val loss: 12234175.875\n",
            "Train loss: 41064.30334472656 and Val loss: 13255193.75\n",
            "Train loss: 6625.6572265625 and Val loss: 1012524.5\n",
            "Train loss: 8222.719360351562 and Val loss: 2028048.8125\n",
            "Train loss: 9813.734375 and Val loss: 3045345.875\n",
            "Train loss: 10711.476989746094 and Val loss: 4061621.625\n",
            "Train loss: 16377.142517089844 and Val loss: 5077040.0\n",
            "Train loss: 17195.734130859375 and Val loss: 6094047.8125\n",
            "Train loss: 19452.3544921875 and Val loss: 7110006.9375\n",
            "Train loss: 21175.132690429688 and Val loss: 8124778.1875\n",
            "Train loss: 22564.612915039062 and Val loss: 9141512.1875\n",
            "Train loss: 24011.6904296875 and Val loss: 10160030.6875\n",
            "Train loss: 25696.387329101562 and Val loss: 11178344.0\n",
            "Train loss: 29249.029174804688 and Val loss: 12196316.25\n",
            "Train loss: 32426.588500976562 and Val loss: 13216036.6875\n",
            "Train loss: 1651.7730712890625 and Val loss: 1022743.0\n",
            "Train loss: 3293.3297119140625 and Val loss: 2043387.3125\n",
            "Train loss: 6014.2125244140625 and Val loss: 3064700.5\n",
            "Train loss: 7341.1435546875 and Val loss: 4083542.3125\n",
            "Train loss: 8458.272094726562 and Val loss: 5103093.4375\n",
            "Train loss: 9541.826171875 and Val loss: 6121907.8125\n",
            "Train loss: 12467.41552734375 and Val loss: 7141038.625\n",
            "Train loss: 13516.273315429688 and Val loss: 8160862.625\n",
            "Train loss: 14733.108276367188 and Val loss: 9182143.4375\n",
            "Train loss: 16998.013305664062 and Val loss: 10203837.5\n",
            "Train loss: 18564.74267578125 and Val loss: 11227582.9375\n",
            "Train loss: 20724.43603515625 and Val loss: 12248843.3125\n",
            "Train loss: 24302.0791015625 and Val loss: 13269265.8125\n",
            "Train loss: 4130.2451171875 and Val loss: 1013561.0625\n",
            "Train loss: 5965.0435791015625 and Val loss: 2027913.4375\n",
            "Train loss: 7864.41943359375 and Val loss: 3041792.1875\n",
            "Train loss: 8442.421691894531 and Val loss: 4060185.6875\n",
            "Train loss: 9755.835388183594 and Val loss: 5076773.25\n",
            "Train loss: 12193.054626464844 and Val loss: 6093844.25\n",
            "Train loss: 16388.048278808594 and Val loss: 7111123.625\n",
            "Train loss: 17037.28485107422 and Val loss: 8130625.875\n",
            "Train loss: 18214.000915527344 and Val loss: 9151699.25\n",
            "Train loss: 19261.341857910156 and Val loss: 10172895.0\n",
            "Train loss: 21060.681091308594 and Val loss: 11188808.375\n",
            "Train loss: 25318.962341308594 and Val loss: 12207905.9375\n",
            "Train loss: 29137.80877685547 and Val loss: 13227446.9375\n",
            "Train loss: 1082.2513427734375 and Val loss: 1021419.4375\n",
            "Train loss: 1996.6453247070312 and Val loss: 2042322.5\n",
            "Train loss: 4538.774719238281 and Val loss: 3062889.6875\n",
            "Train loss: 6776.588928222656 and Val loss: 4083617.0\n",
            "Train loss: 10754.436340332031 and Val loss: 5103854.0\n",
            "Train loss: 12597.396911621094 and Val loss: 6120567.9375\n",
            "Train loss: 14985.925964355469 and Val loss: 7139252.5625\n",
            "Train loss: 17049.458435058594 and Val loss: 8156683.375\n",
            "Train loss: 18901.15020751953 and Val loss: 9172252.75\n",
            "Train loss: 24656.45343017578 and Val loss: 10188164.4375\n",
            "Train loss: 26286.481384277344 and Val loss: 11204352.8125\n",
            "Train loss: 27684.273864746094 and Val loss: 12218923.6875\n",
            "Train loss: 28684.392456054688 and Val loss: 13233048.875\n",
            "Train loss: 1848.066650390625 and Val loss: 1015199.8125\n",
            "Train loss: 9998.455810546875 and Val loss: 2035148.3125\n",
            "Train loss: 10718.630187988281 and Val loss: 3055075.4375\n",
            "Train loss: 11194.039184570312 and Val loss: 4075379.5\n",
            "Train loss: 12685.791625976562 and Val loss: 5097923.3125\n",
            "Train loss: 13684.468994140625 and Val loss: 6122510.125\n",
            "Train loss: 14271.038635253906 and Val loss: 7150202.1875\n",
            "Train loss: 23364.097229003906 and Val loss: 8177558.9375\n",
            "Train loss: 24750.528869628906 and Val loss: 9206223.125\n",
            "Train loss: 31676.361877441406 and Val loss: 10232656.625\n",
            "Train loss: 33376.284606933594 and Val loss: 11257703.625\n",
            "Train loss: 34852.32342529297 and Val loss: 12282742.75\n",
            "Train loss: 42881.01873779297 and Val loss: 13305525.9375\n",
            "Train loss: 1575.3787841796875 and Val loss: 1026932.5\n",
            "Train loss: 2296.0166015625 and Val loss: 2054646.3125\n",
            "Train loss: 3971.9197998046875 and Val loss: 3082230.8125\n",
            "Train loss: 4817.147277832031 and Val loss: 4108515.5\n",
            "Train loss: 6103.594177246094 and Val loss: 5137371.1875\n",
            "Train loss: 7833.323059082031 and Val loss: 6164382.9375\n",
            "Train loss: 12311.370422363281 and Val loss: 7191218.5625\n",
            "Train loss: 14142.089172363281 and Val loss: 8217757.25\n",
            "Train loss: 16148.547058105469 and Val loss: 9246550.8125\n",
            "Train loss: 18071.43634033203 and Val loss: 10276697.0625\n",
            "Train loss: 19133.32440185547 and Val loss: 11303018.875\n",
            "Train loss: 20857.882263183594 and Val loss: 12326668.6875\n",
            "Train loss: 33152.310974121094 and Val loss: 13349853.875\n",
            "Train loss: 1692.5615234375 and Val loss: 1028486.6875\n",
            "Train loss: 11425.732421875 and Val loss: 2059306.75\n",
            "Train loss: 14124.447265625 and Val loss: 3083725.8125\n",
            "Train loss: 16821.914794921875 and Val loss: 4101318.375\n",
            "Train loss: 27675.730224609375 and Val loss: 5124465.625\n",
            "Train loss: 34318.102294921875 and Val loss: 6169207.3125\n",
            "Train loss: 44027.474365234375 and Val loss: 7197468.9375\n",
            "Train loss: 54520.787841796875 and Val loss: 8224921.875\n",
            "Train loss: 56403.2158203125 and Val loss: 9254062.875\n",
            "Train loss: 67003.998046875 and Val loss: 10291888.875\n",
            "Train loss: 70577.80981445312 and Val loss: 11320481.4375\n",
            "Train loss: 75720.12622070312 and Val loss: 12337434.1875\n",
            "Train loss: 77701.91723632812 and Val loss: 13353186.6875\n",
            "Train loss: 2928.168701171875 and Val loss: 1019539.3125\n",
            "Train loss: 4344.0518798828125 and Val loss: 2048269.4375\n",
            "Train loss: 6795.9561767578125 and Val loss: 3070787.5\n",
            "Train loss: 10302.850708007812 and Val loss: 4098754.0625\n",
            "Train loss: 11686.116088867188 and Val loss: 5121860.25\n",
            "Train loss: 14353.240844726562 and Val loss: 6147341.0625\n",
            "Train loss: 20134.628540039062 and Val loss: 7170734.0\n",
            "Train loss: 21140.043090820312 and Val loss: 8185716.25\n",
            "Train loss: 23429.233520507812 and Val loss: 9194182.8125\n",
            "Train loss: 25924.616821289062 and Val loss: 10190757.375\n",
            "Train loss: 26911.615478515625 and Val loss: 11187358.1875\n",
            "Train loss: 29872.649658203125 and Val loss: 12187089.875\n",
            "Train loss: 31723.410278320312 and Val loss: 13186537.4375\n",
            "Train loss: 3403.948974609375 and Val loss: 1007440.625\n",
            "Train loss: 4879.4447021484375 and Val loss: 2007420.1875\n",
            "Train loss: 8976.637084960938 and Val loss: 3009390.4375\n",
            "Train loss: 10067.47412109375 and Val loss: 4009174.25\n",
            "Train loss: 15474.77734375 and Val loss: 5011318.4375\n",
            "Train loss: 17066.069580078125 and Val loss: 6014016.75\n",
            "Train loss: 18429.94580078125 and Val loss: 7015658.6875\n",
            "Train loss: 24999.07568359375 and Val loss: 8025656.9375\n",
            "Train loss: 26305.774169921875 and Val loss: 9035655.625\n",
            "Train loss: 28864.349609375 and Val loss: 10042668.125\n",
            "Train loss: 30640.631713867188 and Val loss: 11048888.5\n",
            "Train loss: 35270.39685058594 and Val loss: 12058605.5625\n",
            "Train loss: 40612.86755371094 and Val loss: 13063054.375\n",
            "Train loss: 1567.6551513671875 and Val loss: 1001558.5625\n",
            "Train loss: 2378.1632690429688 and Val loss: 2006649.5\n",
            "Train loss: 5513.267272949219 and Val loss: 3014742.6875\n",
            "Train loss: 7480.552795410156 and Val loss: 4024516.6875\n",
            "Train loss: 8483.901550292969 and Val loss: 5033110.875\n",
            "Train loss: 13572.423522949219 and Val loss: 6046494.9375\n",
            "Train loss: 14566.016235351562 and Val loss: 7057901.25\n",
            "Train loss: 17825.963989257812 and Val loss: 8070371.1875\n",
            "Train loss: 18349.052001953125 and Val loss: 9078302.8125\n",
            "Train loss: 22531.399658203125 and Val loss: 10091278.6875\n",
            "Train loss: 24010.784301757812 and Val loss: 11105908.3125\n",
            "Train loss: 26465.628540039062 and Val loss: 12123619.625\n",
            "Train loss: 27552.5302734375 and Val loss: 13136652.5625\n",
            "Train loss: 4244.12890625 and Val loss: 1005689.0\n",
            "Train loss: 10398.65576171875 and Val loss: 2012540.6875\n",
            "Train loss: 15324.537109375 and Val loss: 3018290.3125\n",
            "Train loss: 16953.772827148438 and Val loss: 4030230.0625\n",
            "Train loss: 19191.285522460938 and Val loss: 5041047.1875\n",
            "Train loss: 21069.099243164062 and Val loss: 6052041.5625\n",
            "Train loss: 28671.009399414062 and Val loss: 7066508.5625\n",
            "Train loss: 32499.359497070312 and Val loss: 8076580.25\n",
            "Train loss: 35084.38317871094 and Val loss: 9086554.1875\n",
            "Train loss: 36393.69372558594 and Val loss: 10099297.25\n",
            "Train loss: 37662.83044433594 and Val loss: 11109434.3125\n",
            "Train loss: 39930.86047363281 and Val loss: 12115577.5625\n",
            "Train loss: 44098.35852050781 and Val loss: 13125900.8125\n",
            "Train loss: 2698.127685546875 and Val loss: 1021084.875\n",
            "Train loss: 3567.5148315429688 and Val loss: 2035963.875\n",
            "Train loss: 4772.809875488281 and Val loss: 3048366.4375\n",
            "Train loss: 6547.346374511719 and Val loss: 4060384.6875\n",
            "Train loss: 9052.138610839844 and Val loss: 5070475.0625\n",
            "Train loss: 10579.637878417969 and Val loss: 6085332.375\n",
            "Train loss: 11886.979064941406 and Val loss: 7101664.375\n",
            "Train loss: 13133.417785644531 and Val loss: 8122660.625\n",
            "Train loss: 15455.384094238281 and Val loss: 9136750.9375\n",
            "Train loss: 16599.75177001953 and Val loss: 10154653.875\n",
            "Train loss: 18161.83184814453 and Val loss: 11174740.4375\n",
            "Train loss: 19847.92156982422 and Val loss: 12193652.25\n",
            "Train loss: 21869.413635253906 and Val loss: 13214970.125\n",
            "Train loss: 2691.088623046875 and Val loss: 1035942.5\n",
            "Train loss: 4971.276611328125 and Val loss: 2073029.625\n",
            "Train loss: 6165.0113525390625 and Val loss: 3105790.5625\n",
            "Train loss: 9309.995727539062 and Val loss: 4144250.375\n",
            "Train loss: 11144.7216796875 and Val loss: 5184489.5625\n",
            "Train loss: 13400.611083984375 and Val loss: 6221971.0625\n",
            "Train loss: 15029.784301757812 and Val loss: 7260298.9375\n",
            "Train loss: 16271.878662109375 and Val loss: 8306356.0\n",
            "Train loss: 24424.386474609375 and Val loss: 9347497.1875\n",
            "Train loss: 27660.268310546875 and Val loss: 10379831.625\n",
            "Train loss: 30858.08544921875 and Val loss: 11407859.125\n",
            "Train loss: 33233.067626953125 and Val loss: 12439388.9375\n",
            "Train loss: 35492.89892578125 and Val loss: 13466505.4375\n",
            "Train loss: 1835.7581787109375 and Val loss: 1021766.5\n",
            "Train loss: 5334.4639892578125 and Val loss: 2044799.8125\n",
            "Train loss: 6791.753173828125 and Val loss: 3072440.1875\n",
            "Train loss: 9202.84814453125 and Val loss: 4104234.875\n",
            "Train loss: 11613.942138671875 and Val loss: 5138232.125\n",
            "Train loss: 13161.518310546875 and Val loss: 6174947.625\n",
            "Train loss: 21147.394775390625 and Val loss: 7217731.0\n",
            "Train loss: 22609.306640625 and Val loss: 8256414.5\n",
            "Train loss: 25888.382080078125 and Val loss: 9301320.1875\n",
            "Train loss: 33209.343994140625 and Val loss: 10343488.3125\n",
            "Train loss: 35519.2412109375 and Val loss: 11388650.625\n",
            "Train loss: 40264.041015625 and Val loss: 12424333.5625\n",
            "Train loss: 41023.067810058594 and Val loss: 13459470.75\n",
            "Train loss: 1760.886962890625 and Val loss: 1030226.0\n",
            "Train loss: 3028.0965576171875 and Val loss: 2063155.1875\n",
            "Train loss: 4932.0596923828125 and Val loss: 3095190.625\n",
            "Train loss: 10114.951782226562 and Val loss: 4128408.9375\n",
            "Train loss: 12745.497924804688 and Val loss: 5165244.125\n",
            "Train loss: 14564.276245117188 and Val loss: 6207046.25\n",
            "Train loss: 15940.167724609375 and Val loss: 7241166.9375\n",
            "Train loss: 25044.563232421875 and Val loss: 8286360.4375\n",
            "Train loss: 28595.646484375 and Val loss: 9326239.5\n",
            "Train loss: 31414.592529296875 and Val loss: 10372788.625\n",
            "Train loss: 32498.483032226562 and Val loss: 11414692.0\n",
            "Train loss: 36264.29162597656 and Val loss: 12462481.5\n",
            "Train loss: 38635.66125488281 and Val loss: 13501682.5625\n",
            "Train loss: 1862.658203125 and Val loss: 1019620.6875\n",
            "Train loss: 3513.718994140625 and Val loss: 2027223.5\n",
            "Train loss: 7025.570556640625 and Val loss: 3044876.6875\n",
            "Train loss: 30940.635009765625 and Val loss: 4071179.9375\n",
            "Train loss: 33534.61328125 and Val loss: 5102209.25\n",
            "Train loss: 35180.93322753906 and Val loss: 6141160.375\n",
            "Train loss: 37417.64416503906 and Val loss: 7187839.625\n",
            "Train loss: 40876.11267089844 and Val loss: 8226705.125\n",
            "Train loss: 49233.83337402344 and Val loss: 9268297.125\n",
            "Train loss: 52504.21008300781 and Val loss: 10308310.8125\n",
            "Train loss: 57527.86047363281 and Val loss: 11348137.0625\n",
            "Train loss: 58826.13098144531 and Val loss: 12380578.9375\n",
            "Train loss: 63155.15686035156 and Val loss: 13407602.625\n",
            "Train loss: 1927.3739013671875 and Val loss: 1031186.5\n",
            "Train loss: 4125.7340087890625 and Val loss: 2054180.0\n",
            "Train loss: 5145.946716308594 and Val loss: 3087053.1875\n",
            "Train loss: 5958.6448974609375 and Val loss: 4115678.6875\n",
            "Train loss: 6445.891693115234 and Val loss: 5150382.5625\n",
            "Train loss: 9203.68588256836 and Val loss: 6184385.75\n",
            "Train loss: 10362.529266357422 and Val loss: 7218596.625\n",
            "Train loss: 12031.97152709961 and Val loss: 8248429.4375\n",
            "Train loss: 14332.367767333984 and Val loss: 9283201.1875\n",
            "Train loss: 17883.63851928711 and Val loss: 10314464.5625\n",
            "Train loss: 19066.899505615234 and Val loss: 11348521.125\n",
            "Train loss: 21151.94125366211 and Val loss: 12377226.25\n",
            "Train loss: 27132.24691772461 and Val loss: 13409327.6875\n",
            "Train loss: 1631.1546630859375 and Val loss: 1041219.0\n",
            "Train loss: 2880.479736328125 and Val loss: 2089209.3125\n",
            "Train loss: 6730.46337890625 and Val loss: 3128138.3125\n",
            "Train loss: 10408.7626953125 and Val loss: 4175425.0625\n",
            "Train loss: 18090.490234375 and Val loss: 5219131.375\n",
            "Train loss: 18521.216369628906 and Val loss: 6262852.6875\n",
            "Train loss: 24988.315490722656 and Val loss: 7298712.5\n",
            "Train loss: 26330.33416748047 and Val loss: 8335782.125\n",
            "Train loss: 28083.73651123047 and Val loss: 9361991.75\n",
            "Train loss: 30116.32440185547 and Val loss: 10379359.875\n",
            "Train loss: 34665.79266357422 and Val loss: 11395509.875\n",
            "Train loss: 36212.868225097656 and Val loss: 12419746.375\n",
            "Train loss: 37580.657165527344 and Val loss: 13456026.125\n",
            "Train loss: 711.0287475585938 and Val loss: 1037745.8125\n",
            "Train loss: 3801.4884643554688 and Val loss: 2086476.5625\n",
            "Train loss: 4679.1014404296875 and Val loss: 3132789.8125\n",
            "Train loss: 7168.2987060546875 and Val loss: 4177801.0\n",
            "Train loss: 13038.307983398438 and Val loss: 5224886.0625\n",
            "Train loss: 16483.647827148438 and Val loss: 6271605.6875\n",
            "Train loss: 17407.37255859375 and Val loss: 7312545.1875\n",
            "Train loss: 18582.184936523438 and Val loss: 8360624.375\n",
            "Train loss: 20876.438842773438 and Val loss: 9404132.875\n",
            "Train loss: 23879.029418945312 and Val loss: 10457746.75\n",
            "Train loss: 24412.995727539062 and Val loss: 11512068.125\n",
            "Train loss: 26073.003173828125 and Val loss: 12559485.5\n",
            "Train loss: 28458.26904296875 and Val loss: 13599028.875\n",
            "Train loss: 1411.4527587890625 and Val loss: 1029406.4375\n",
            "Train loss: 2361.827392578125 and Val loss: 2056032.75\n",
            "Train loss: 3987.4945068359375 and Val loss: 3081133.625\n",
            "Train loss: 5367.36376953125 and Val loss: 4107047.5\n",
            "Train loss: 7219.07275390625 and Val loss: 5127919.3125\n",
            "Train loss: 8040.863098144531 and Val loss: 6149269.3125\n",
            "Train loss: 8994.822692871094 and Val loss: 7165373.1875\n",
            "Train loss: 10845.879333496094 and Val loss: 8181628.375\n",
            "Train loss: 14583.348815917969 and Val loss: 9196415.8125\n",
            "Train loss: 15387.377136230469 and Val loss: 10214292.5625\n",
            "Train loss: 16701.228576660156 and Val loss: 11230345.3125\n",
            "Train loss: 17720.038330078125 and Val loss: 12250894.125\n",
            "Train loss: 20474.249267578125 and Val loss: 13274740.5625\n",
            "Train loss: 1117.551025390625 and Val loss: 1033455.375\n",
            "Train loss: 2537.8494873046875 and Val loss: 2063058.5625\n",
            "Train loss: 3255.5048828125 and Val loss: 3093404.75\n",
            "Train loss: 4993.3236083984375 and Val loss: 4118803.0625\n",
            "Train loss: 6794.745361328125 and Val loss: 5143092.125\n",
            "Train loss: 7837.152099609375 and Val loss: 6167554.5625\n",
            "Train loss: 11271.54052734375 and Val loss: 7187387.9375\n",
            "Train loss: 14416.423095703125 and Val loss: 8210030.125\n",
            "Train loss: 16011.968994140625 and Val loss: 9237335.625\n",
            "Train loss: 17866.725219726562 and Val loss: 10267475.375\n",
            "Train loss: 18768.568298339844 and Val loss: 11298225.6875\n",
            "Train loss: 20820.75946044922 and Val loss: 12326286.875\n",
            "Train loss: 25405.80487060547 and Val loss: 13355680.875\n",
            "Train loss: 2981.8271484375 and Val loss: 1044662.4375\n",
            "Train loss: 4183.1707763671875 and Val loss: 2077766.25\n",
            "Train loss: 5632.585205078125 and Val loss: 3113884.9375\n",
            "Train loss: 9790.172119140625 and Val loss: 4145140.625\n",
            "Train loss: 11473.52197265625 and Val loss: 5183164.125\n",
            "Train loss: 15551.74462890625 and Val loss: 6206035.375\n",
            "Train loss: 18760.65283203125 and Val loss: 7235087.875\n",
            "Train loss: 20453.076538085938 and Val loss: 8246994.6875\n",
            "Train loss: 21328.9189453125 and Val loss: 9253905.9375\n",
            "Train loss: 25169.927490234375 and Val loss: 10279468.875\n",
            "Train loss: 26553.66650390625 and Val loss: 11303579.1875\n",
            "Train loss: 27336.16766357422 and Val loss: 12330960.25\n",
            "Train loss: 30178.626403808594 and Val loss: 13367027.625\n",
            "Train loss: 1300.7633056640625 and Val loss: 1044226.5625\n",
            "Train loss: 3788.2711181640625 and Val loss: 2078937.9375\n",
            "Train loss: 4589.7598876953125 and Val loss: 3113355.3125\n",
            "Train loss: 5773.1055908203125 and Val loss: 4155961.5\n",
            "Train loss: 6875.0582275390625 and Val loss: 5197783.0625\n",
            "Train loss: 7912.734619140625 and Val loss: 6228120.875\n",
            "Train loss: 11017.988037109375 and Val loss: 7251374.75\n",
            "Train loss: 12041.895202636719 and Val loss: 8285516.5\n",
            "Train loss: 13172.079162597656 and Val loss: 9323906.0625\n",
            "Train loss: 14391.066467285156 and Val loss: 10363362.1875\n",
            "Train loss: 15188.500549316406 and Val loss: 11406931.875\n",
            "Train loss: 16443.28692626953 and Val loss: 12443289.6875\n",
            "Train loss: 19170.476135253906 and Val loss: 13479296.5\n",
            "Train loss: 2578.599365234375 and Val loss: 1031749.9375\n",
            "Train loss: 3937.102294921875 and Val loss: 2082901.5625\n",
            "Train loss: 5205.19140625 and Val loss: 3126952.875\n",
            "Train loss: 6146.0113525390625 and Val loss: 4171307.625\n",
            "Train loss: 8040.837890625 and Val loss: 5209223.5\n",
            "Train loss: 8767.558532714844 and Val loss: 6250907.5\n",
            "Train loss: 10840.856140136719 and Val loss: 7284257.3125\n",
            "Train loss: 13419.342224121094 and Val loss: 8326640.4375\n",
            "Train loss: 14638.618225097656 and Val loss: 9369082.5\n",
            "Train loss: 16172.638244628906 and Val loss: 10420040.25\n",
            "Train loss: 17601.133728027344 and Val loss: 11470128.75\n",
            "Train loss: 18244.569396972656 and Val loss: 12513141.3125\n",
            "Train loss: 19055.49334716797 and Val loss: 13553270.4375\n",
            "Train loss: 2447.718505859375 and Val loss: 1047814.375\n",
            "Train loss: 3335.898193359375 and Val loss: 2094696.1875\n",
            "Train loss: 4680.8350830078125 and Val loss: 3140846.3125\n",
            "Train loss: 5553.5723876953125 and Val loss: 4191957.6875\n",
            "Train loss: 8227.454711914062 and Val loss: 5242769.6875\n",
            "Train loss: 18953.953735351562 and Val loss: 6286768.0\n",
            "Train loss: 19864.629943847656 and Val loss: 7344483.75\n",
            "Train loss: 20748.4853515625 and Val loss: 8402393.625\n",
            "Train loss: 23401.05859375 and Val loss: 9443587.75\n",
            "Train loss: 24838.866821289062 and Val loss: 10481204.0625\n",
            "Train loss: 26971.287719726562 and Val loss: 11527871.8125\n",
            "Train loss: 32700.964965820312 and Val loss: 12580718.1875\n",
            "Train loss: 34560.21728515625 and Val loss: 13624888.625\n",
            "Train loss: 1633.4493408203125 and Val loss: 1043399.0\n",
            "Train loss: 3944.3450927734375 and Val loss: 2086265.8125\n",
            "Train loss: 5965.0045166015625 and Val loss: 3130218.8125\n",
            "Train loss: 8325.079956054688 and Val loss: 4169046.3125\n",
            "Train loss: 9270.703002929688 and Val loss: 5210960.0\n",
            "Train loss: 11862.527709960938 and Val loss: 6249892.1875\n",
            "Train loss: 14163.860717773438 and Val loss: 7299686.1875\n",
            "Train loss: 17075.149047851562 and Val loss: 8347546.0\n",
            "Train loss: 18558.40966796875 and Val loss: 9398829.375\n",
            "Train loss: 19440.83270263672 and Val loss: 10454586.0\n",
            "Train loss: 21744.943786621094 and Val loss: 11509148.375\n",
            "Train loss: 28611.976501464844 and Val loss: 12558152.25\n",
            "Train loss: 33763.762145996094 and Val loss: 13622375.625\n",
            "Train loss: 1262.9033203125 and Val loss: 1054283.75\n",
            "Train loss: 2153.4890747070312 and Val loss: 2093991.0\n",
            "Train loss: 3665.4523315429688 and Val loss: 3141433.0625\n",
            "Train loss: 5975.949401855469 and Val loss: 4182732.6875\n",
            "Train loss: 7731.168029785156 and Val loss: 5228081.4375\n",
            "Train loss: 10378.289123535156 and Val loss: 6272329.8125\n",
            "Train loss: 13412.663391113281 and Val loss: 7319712.375\n",
            "Train loss: 15352.505798339844 and Val loss: 8361193.375\n",
            "Train loss: 19221.461364746094 and Val loss: 9402234.375\n",
            "Train loss: 21217.22198486328 and Val loss: 10433861.8125\n",
            "Train loss: 22892.543884277344 and Val loss: 11456218.75\n",
            "Train loss: 24828.821228027344 and Val loss: 12485075.6875\n",
            "Train loss: 27049.85565185547 and Val loss: 13513555.6875\n",
            "Train loss: 797.2116088867188 and Val loss: 1055299.875\n",
            "Train loss: 1714.6500854492188 and Val loss: 2107440.75\n",
            "Train loss: 2916.1005249023438 and Val loss: 3157740.75\n",
            "Train loss: 4103.310852050781 and Val loss: 4209465.75\n",
            "Train loss: 5687.550354003906 and Val loss: 5266830.0\n",
            "Train loss: 8226.586730957031 and Val loss: 6318947.75\n",
            "Train loss: 10270.716003417969 and Val loss: 7367168.0625\n",
            "Train loss: 12003.212707519531 and Val loss: 8426538.6875\n",
            "Train loss: 12894.594970703125 and Val loss: 9476104.4375\n",
            "Train loss: 14284.960815429688 and Val loss: 10523634.8125\n",
            "Train loss: 15521.825317382812 and Val loss: 11574804.3125\n",
            "Train loss: 17895.476928710938 and Val loss: 12628591.0625\n",
            "Train loss: 19168.455810546875 and Val loss: 13688125.3125\n",
            "Train loss: 2594.29833984375 and Val loss: 1049032.375\n",
            "Train loss: 5362.95263671875 and Val loss: 2091236.0\n",
            "Train loss: 7144.750732421875 and Val loss: 3133477.4375\n",
            "Train loss: 8563.811157226562 and Val loss: 4172145.3125\n",
            "Train loss: 10618.390014648438 and Val loss: 5214699.625\n",
            "Train loss: 13779.065063476562 and Val loss: 6257059.375\n",
            "Train loss: 19714.777465820312 and Val loss: 7310720.0\n",
            "Train loss: 21467.511474609375 and Val loss: 8367069.625\n",
            "Train loss: 23408.1044921875 and Val loss: 9422941.5\n",
            "Train loss: 25517.7177734375 and Val loss: 10489458.0\n",
            "Train loss: 27282.958984375 and Val loss: 11552273.375\n",
            "Train loss: 32955.490234375 and Val loss: 12606199.25\n",
            "Train loss: 34576.75402832031 and Val loss: 13661848.375\n",
            "Train loss: 2986.432373046875 and Val loss: 1031284.875\n",
            "Train loss: 3765.3524780273438 and Val loss: 2036535.0625\n",
            "Train loss: 5268.537902832031 and Val loss: 3055137.1875\n",
            "Train loss: 9668.032531738281 and Val loss: 4076196.75\n",
            "Train loss: 10082.842071533203 and Val loss: 5107084.5625\n",
            "Train loss: 11378.84292602539 and Val loss: 6132890.625\n",
            "Train loss: 19380.94790649414 and Val loss: 7177659.5625\n",
            "Train loss: 22115.215240478516 and Val loss: 8198653.75\n",
            "Train loss: 26492.675689697266 and Val loss: 9232072.75\n",
            "Train loss: 29356.879302978516 and Val loss: 10273625.625\n",
            "Train loss: 30642.205963134766 and Val loss: 11304465.5625\n",
            "Train loss: 32279.10659790039 and Val loss: 12331570.6875\n",
            "Train loss: 36443.84976196289 and Val loss: 13347316.9375\n",
            "Train loss: 1192.326171875 and Val loss: 1016646.375\n",
            "Train loss: 5838.498046875 and Val loss: 2048711.1875\n",
            "Train loss: 6593.2099609375 and Val loss: 3081890.9375\n",
            "Train loss: 8242.622924804688 and Val loss: 4126528.25\n",
            "Train loss: 9555.536376953125 and Val loss: 5169529.6875\n",
            "Train loss: 11175.28759765625 and Val loss: 6203160.0\n",
            "Train loss: 15430.70654296875 and Val loss: 7241035.6875\n",
            "Train loss: 19449.38232421875 and Val loss: 8284955.5625\n",
            "Train loss: 21457.620239257812 and Val loss: 9329550.8125\n",
            "Train loss: 23970.484252929688 and Val loss: 10360370.875\n",
            "Train loss: 25024.028686523438 and Val loss: 11388610.0\n",
            "Train loss: 29357.855346679688 and Val loss: 12374770.125\n",
            "Train loss: 36153.96228027344 and Val loss: 13400408.6875\n",
            "Train loss: 1046.6998291015625 and Val loss: 1009723.0\n",
            "Train loss: 2809.1383056640625 and Val loss: 2032252.625\n",
            "Train loss: 5117.3472900390625 and Val loss: 3063299.625\n",
            "Train loss: 6152.509521484375 and Val loss: 4079682.5625\n",
            "Train loss: 7176.114685058594 and Val loss: 5093585.125\n",
            "Train loss: 8799.958679199219 and Val loss: 6105595.9375\n",
            "Train loss: 10998.684020996094 and Val loss: 7123926.125\n",
            "Train loss: 13881.926940917969 and Val loss: 8145763.1875\n",
            "Train loss: 15359.703430175781 and Val loss: 9158387.25\n",
            "Train loss: 16840.716857910156 and Val loss: 10173225.4375\n",
            "Train loss: 19196.998596191406 and Val loss: 11202536.125\n",
            "Train loss: 20918.315002441406 and Val loss: 12221933.5\n",
            "Train loss: 22512.26788330078 and Val loss: 13253550.625\n",
            "Train loss: 9403.6552734375 and Val loss: 1064732.875\n",
            "Train loss: 11349.933227539062 and Val loss: 2134539.25\n",
            "Train loss: 12426.607055664062 and Val loss: 3197415.125\n",
            "Train loss: 15886.391479492188 and Val loss: 4262956.5\n",
            "Train loss: 17134.419189453125 and Val loss: 5334642.5\n",
            "Train loss: 18248.959106445312 and Val loss: 6394836.0\n",
            "Train loss: 19698.6884765625 and Val loss: 7451822.5\n",
            "Train loss: 20652.479431152344 and Val loss: 8495077.625\n",
            "Train loss: 22127.181579589844 and Val loss: 9532318.75\n",
            "Train loss: 24255.152282714844 and Val loss: 10571718.875\n",
            "Train loss: 26188.295471191406 and Val loss: 11619931.1875\n",
            "Train loss: 28917.915100097656 and Val loss: 12676719.4375\n",
            "Train loss: 33747.450256347656 and Val loss: 13735088.9375\n",
            "Train loss: 1516.79833984375 and Val loss: 1013906.875\n",
            "Train loss: 6758.59521484375 and Val loss: 2045151.0\n",
            "Train loss: 7455.817199707031 and Val loss: 3082791.9375\n",
            "Train loss: 12904.882629394531 and Val loss: 4137532.4375\n",
            "Train loss: 13587.468688964844 and Val loss: 5183732.0625\n",
            "Train loss: 15343.800964355469 and Val loss: 6231862.0625\n",
            "Train loss: 17473.108337402344 and Val loss: 7283231.5625\n",
            "Train loss: 19006.09588623047 and Val loss: 8338171.3125\n",
            "Train loss: 20655.307678222656 and Val loss: 9382825.375\n",
            "Train loss: 25491.615295410156 and Val loss: 10436626.0\n",
            "Train loss: 27017.344177246094 and Val loss: 11484565.0\n",
            "Train loss: 28000.708862304688 and Val loss: 12530384.5\n",
            "Train loss: 28811.72607421875 and Val loss: 13570011.8125\n",
            "Train loss: 2008.6591796875 and Val loss: 1053783.75\n",
            "Train loss: 3161.25634765625 and Val loss: 2114308.5\n",
            "Train loss: 4839.911865234375 and Val loss: 3171687.125\n",
            "Train loss: 12611.264404296875 and Val loss: 4222579.625\n",
            "Train loss: 14295.243286132812 and Val loss: 5273100.875\n",
            "Train loss: 17765.785522460938 and Val loss: 6309925.9375\n",
            "Train loss: 20372.749633789062 and Val loss: 7355574.5625\n",
            "Train loss: 22213.661010742188 and Val loss: 8370414.0\n",
            "Train loss: 23896.574096679688 and Val loss: 9386668.75\n",
            "Train loss: 26297.760864257812 and Val loss: 10379508.0625\n",
            "Train loss: 28155.730834960938 and Val loss: 11387075.75\n",
            "Train loss: 35014.08093261719 and Val loss: 12407822.1875\n",
            "Train loss: 38174.94030761719 and Val loss: 13443177.125\n",
            "Train loss: 1926.9320068359375 and Val loss: 1010845.875\n",
            "Train loss: 3763.8031005859375 and Val loss: 2028180.75\n",
            "Train loss: 5251.145263671875 and Val loss: 3047972.8125\n",
            "Train loss: 6983.4508056640625 and Val loss: 4081215.25\n",
            "Train loss: 8226.901611328125 and Val loss: 5108418.375\n",
            "Train loss: 10032.491088867188 and Val loss: 6139883.0\n",
            "Train loss: 11724.183471679688 and Val loss: 7176443.9375\n",
            "Train loss: 12647.060546875 and Val loss: 8199896.8125\n",
            "Train loss: 15613.425048828125 and Val loss: 9231021.0\n",
            "Train loss: 16102.542572021484 and Val loss: 10256329.3125\n",
            "Train loss: 17462.338470458984 and Val loss: 11298967.125\n",
            "Train loss: 22140.442962646484 and Val loss: 12323795.375\n",
            "Train loss: 22967.392120361328 and Val loss: 13350102.5\n",
            "Train loss: 727.2294311523438 and Val loss: 1033216.625\n",
            "Train loss: 2506.3087768554688 and Val loss: 2062616.3125\n",
            "Train loss: 4077.9260864257812 and Val loss: 3080212.9375\n",
            "Train loss: 6493.807678222656 and Val loss: 4092791.3125\n",
            "Train loss: 8687.626770019531 and Val loss: 5108536.6875\n",
            "Train loss: 10372.022644042969 and Val loss: 6134751.4375\n",
            "Train loss: 17697.13201904297 and Val loss: 7166539.5\n",
            "Train loss: 19454.918395996094 and Val loss: 8203123.6875\n",
            "Train loss: 20572.121154785156 and Val loss: 9245600.4375\n",
            "Train loss: 21831.89569091797 and Val loss: 10286477.75\n",
            "Train loss: 23780.947326660156 and Val loss: 11321882.0625\n",
            "Train loss: 26710.31866455078 and Val loss: 12360145.6875\n",
            "Train loss: 28455.009338378906 and Val loss: 13394715.1875\n",
            "Train loss: 5353.00146484375 and Val loss: 1036808.375\n",
            "Train loss: 10159.9677734375 and Val loss: 2077249.3125\n",
            "Train loss: 11565.278686523438 and Val loss: 3121760.3125\n",
            "Train loss: 13199.214721679688 and Val loss: 4149977.1875\n",
            "Train loss: 15095.662353515625 and Val loss: 5189812.0625\n",
            "Train loss: 17044.395629882812 and Val loss: 6225135.125\n",
            "Train loss: 18240.757202148438 and Val loss: 7253400.625\n",
            "Train loss: 23423.957885742188 and Val loss: 8289293.8125\n",
            "Train loss: 27615.861694335938 and Val loss: 9335438.1875\n",
            "Train loss: 29103.571899414062 and Val loss: 10378829.25\n",
            "Train loss: 31158.532592773438 and Val loss: 11418699.0\n",
            "Train loss: 32328.472778320312 and Val loss: 12462594.1875\n",
            "Train loss: 32895.0634765625 and Val loss: 13502307.5\n",
            "Train loss: 599.2732543945312 and Val loss: 1033422.125\n",
            "Train loss: 2072.3191528320312 and Val loss: 2061756.6875\n",
            "Train loss: 2650.3363037109375 and Val loss: 3096235.25\n",
            "Train loss: 4037.495361328125 and Val loss: 4140766.375\n",
            "Train loss: 4867.680725097656 and Val loss: 5185176.0625\n",
            "Train loss: 6592.299255371094 and Val loss: 6219331.6875\n",
            "Train loss: 8257.668762207031 and Val loss: 7250146.0\n",
            "Train loss: 9106.119934082031 and Val loss: 8285470.0\n",
            "Train loss: 10811.234558105469 and Val loss: 9318304.625\n",
            "Train loss: 13077.336853027344 and Val loss: 10362631.0\n",
            "Train loss: 13911.437255859375 and Val loss: 11396051.125\n",
            "Train loss: 15382.76806640625 and Val loss: 12440884.9375\n",
            "Train loss: 24751.39404296875 and Val loss: 13485697.8125\n",
            "Train loss: 616.1154174804688 and Val loss: 1024837.5625\n",
            "Train loss: 1815.5363159179688 and Val loss: 2058392.0625\n",
            "Train loss: 3604.6632690429688 and Val loss: 3093097.0\n",
            "Train loss: 4316.946716308594 and Val loss: 4126998.3125\n",
            "Train loss: 5609.740051269531 and Val loss: 5163360.8125\n",
            "Train loss: 6366.6556396484375 and Val loss: 6197082.6875\n",
            "Train loss: 8487.213012695312 and Val loss: 7243400.6875\n",
            "Train loss: 11061.910034179688 and Val loss: 8283477.375\n",
            "Train loss: 12625.813232421875 and Val loss: 9315824.125\n",
            "Train loss: 16575.24951171875 and Val loss: 10352950.4375\n",
            "Train loss: 17188.745056152344 and Val loss: 11383064.6875\n",
            "Train loss: 19634.125427246094 and Val loss: 12422159.0\n",
            "Train loss: 25803.757751464844 and Val loss: 13473292.625\n",
            "Train loss: 1173.7081298828125 and Val loss: 1047063.0625\n",
            "Train loss: 4969.2803955078125 and Val loss: 2101299.9375\n",
            "Train loss: 5907.1070556640625 and Val loss: 3153794.0625\n",
            "Train loss: 7029.8681640625 and Val loss: 4205847.3125\n",
            "Train loss: 8215.624877929688 and Val loss: 5258202.9375\n",
            "Train loss: 10395.746215820312 and Val loss: 6301483.875\n",
            "Train loss: 13123.089965820312 and Val loss: 7340699.9375\n",
            "Train loss: 14238.7021484375 and Val loss: 8392520.5625\n",
            "Train loss: 14707.559478759766 and Val loss: 9445422.8125\n",
            "Train loss: 15889.829376220703 and Val loss: 10498383.8125\n",
            "Train loss: 17585.678253173828 and Val loss: 11543995.3125\n",
            "Train loss: 19414.07827758789 and Val loss: 12595687.0625\n",
            "Train loss: 21776.79702758789 and Val loss: 13649532.1875\n",
            "Train loss: 1783.6531982421875 and Val loss: 1044608.875\n",
            "Train loss: 2785.1035766601562 and Val loss: 2080578.875\n",
            "Train loss: 4399.476501464844 and Val loss: 3122643.9375\n",
            "Train loss: 6144.670227050781 and Val loss: 4155625.0\n",
            "Train loss: 7521.137390136719 and Val loss: 5185704.1875\n",
            "Train loss: 10102.387878417969 and Val loss: 6206400.4375\n",
            "Train loss: 36844.89764404297 and Val loss: 7243155.75\n",
            "Train loss: 37964.56182861328 and Val loss: 8295048.0\n",
            "Train loss: 39670.74041748047 and Val loss: 9338634.1875\n",
            "Train loss: 40721.70379638672 and Val loss: 10375164.875\n",
            "Train loss: 42098.976135253906 and Val loss: 11421703.5\n",
            "Train loss: 44603.75030517578 and Val loss: 12468888.6875\n",
            "Train loss: 49688.33966064453 and Val loss: 13511918.9375\n",
            "Train loss: 1717.830078125 and Val loss: 1063673.0\n",
            "Train loss: 2991.3148193359375 and Val loss: 2133932.125\n",
            "Train loss: 3769.8758544921875 and Val loss: 3210678.125\n",
            "Train loss: 4938.5015869140625 and Val loss: 4277443.75\n",
            "Train loss: 6808.344970703125 and Val loss: 5348749.75\n",
            "Train loss: 8699.727783203125 and Val loss: 6420084.5\n",
            "Train loss: 9608.234130859375 and Val loss: 7485215.5\n",
            "Train loss: 10010.206512451172 and Val loss: 8547462.0\n",
            "Train loss: 12542.650115966797 and Val loss: 9617023.875\n",
            "Train loss: 15177.904754638672 and Val loss: 10680863.25\n",
            "Train loss: 17168.646575927734 and Val loss: 11748390.0\n",
            "Train loss: 20138.822845458984 and Val loss: 12809108.25\n",
            "Train loss: 20842.730377197266 and Val loss: 13864884.0\n",
            "Train loss: 1513.239501953125 and Val loss: 1053332.25\n",
            "Train loss: 2202.4173583984375 and Val loss: 2090850.125\n",
            "Train loss: 3684.068115234375 and Val loss: 3116673.0\n",
            "Train loss: 5168.1220703125 and Val loss: 4153196.75\n",
            "Train loss: 6106.127624511719 and Val loss: 5200129.0625\n",
            "Train loss: 7962.857727050781 and Val loss: 6235785.25\n",
            "Train loss: 23186.27667236328 and Val loss: 7278809.25\n",
            "Train loss: 24569.857482910156 and Val loss: 8321561.5625\n",
            "Train loss: 25315.00341796875 and Val loss: 9359788.4375\n",
            "Train loss: 26306.402282714844 and Val loss: 10384826.8125\n",
            "Train loss: 27803.379943847656 and Val loss: 11435594.8125\n",
            "Train loss: 30282.97442626953 and Val loss: 12466251.75\n",
            "Train loss: 33575.990783691406 and Val loss: 13511854.5\n",
            "Train loss: 1006.8037109375 and Val loss: 1043230.5625\n",
            "Train loss: 1969.0992431640625 and Val loss: 2099339.1875\n",
            "Train loss: 4447.6707763671875 and Val loss: 3149350.0625\n",
            "Train loss: 7514.9454345703125 and Val loss: 4200484.6875\n",
            "Train loss: 11659.718383789062 and Val loss: 5248929.25\n",
            "Train loss: 13328.348876953125 and Val loss: 6295710.8125\n",
            "Train loss: 15537.87109375 and Val loss: 7361332.9375\n",
            "Train loss: 16635.213745117188 and Val loss: 8419534.8125\n",
            "Train loss: 17284.814025878906 and Val loss: 9487408.5625\n",
            "Train loss: 19101.898864746094 and Val loss: 10547508.3125\n",
            "Train loss: 21288.526306152344 and Val loss: 11604119.1875\n",
            "Train loss: 22758.33721923828 and Val loss: 12659630.3125\n",
            "Train loss: 26495.72003173828 and Val loss: 13728628.1875\n",
            "Train loss: 608.7138061523438 and Val loss: 1066912.0\n",
            "Train loss: 2481.8435668945312 and Val loss: 2130140.0\n",
            "Train loss: 3153.4779663085938 and Val loss: 3193209.375\n",
            "Train loss: 5752.763854980469 and Val loss: 4261854.5\n",
            "Train loss: 6573.5296630859375 and Val loss: 5322164.875\n",
            "Train loss: 7492.4794921875 and Val loss: 6390096.25\n",
            "Train loss: 10415.03515625 and Val loss: 7457565.0\n",
            "Train loss: 10845.942932128906 and Val loss: 8527057.25\n",
            "Train loss: 11669.486328125 and Val loss: 9593675.125\n",
            "Train loss: 14696.541748046875 and Val loss: 10659892.0\n",
            "Train loss: 15979.58203125 and Val loss: 11725926.0\n",
            "Train loss: 16891.60723876953 and Val loss: 12780900.375\n",
            "Train loss: 17828.403442382812 and Val loss: 13824508.1875\n",
            "Train loss: 1133.9163818359375 and Val loss: 1053061.0\n",
            "Train loss: 2817.250244140625 and Val loss: 2101520.4375\n",
            "Train loss: 3300.2354431152344 and Val loss: 3154476.3125\n",
            "Train loss: 4470.975799560547 and Val loss: 4199497.1875\n",
            "Train loss: 5528.433563232422 and Val loss: 5265328.6875\n",
            "Train loss: 6202.791168212891 and Val loss: 6320758.6875\n",
            "Train loss: 7357.119659423828 and Val loss: 7375012.3125\n",
            "Train loss: 8434.147979736328 and Val loss: 8427213.4375\n",
            "Train loss: 8847.490264892578 and Val loss: 9478272.5625\n",
            "Train loss: 9690.53384399414 and Val loss: 10539906.9375\n",
            "Train loss: 10721.913604736328 and Val loss: 11594251.5625\n",
            "Train loss: 12397.255645751953 and Val loss: 12649323.6875\n",
            "Train loss: 14597.208526611328 and Val loss: 13713553.0625\n",
            "Train loss: 1580.9542236328125 and Val loss: 1058329.875\n",
            "Train loss: 3105.708251953125 and Val loss: 2117181.25\n",
            "Train loss: 4862.3897705078125 and Val loss: 3167333.0\n",
            "Train loss: 8486.456176757812 and Val loss: 4227437.375\n",
            "Train loss: 11764.400756835938 and Val loss: 5289614.5\n",
            "Train loss: 13098.254638671875 and Val loss: 6332280.625\n",
            "Train loss: 15660.237548828125 and Val loss: 7383564.125\n",
            "Train loss: 17576.064331054688 and Val loss: 8428801.0625\n",
            "Train loss: 19284.186645507812 and Val loss: 9468333.0\n",
            "Train loss: 20388.844848632812 and Val loss: 10506734.9375\n",
            "Train loss: 21366.942016601562 and Val loss: 11546560.3125\n",
            "Train loss: 22639.068237304688 and Val loss: 12588366.125\n",
            "Train loss: 23784.904052734375 and Val loss: 13630375.8125\n",
            "Train loss: 944.4613647460938 and Val loss: 1040666.25\n",
            "Train loss: 2353.1123657226562 and Val loss: 2079569.3125\n",
            "Train loss: 3578.8621215820312 and Val loss: 3125864.1875\n",
            "Train loss: 4456.888427734375 and Val loss: 4158059.875\n",
            "Train loss: 5457.1976318359375 and Val loss: 5200767.25\n",
            "Train loss: 7288.8323974609375 and Val loss: 6242966.5\n",
            "Train loss: 9324.891845703125 and Val loss: 7291771.875\n",
            "Train loss: 10411.579833984375 and Val loss: 8316634.5625\n",
            "Train loss: 11101.786071777344 and Val loss: 9354486.25\n",
            "Train loss: 12536.900695800781 and Val loss: 10392406.3125\n",
            "Train loss: 14018.545593261719 and Val loss: 11424796.125\n",
            "Train loss: 20136.58270263672 and Val loss: 12462714.625\n",
            "Train loss: 21147.99737548828 and Val loss: 13507473.75\n",
            "Train loss: 509.1925354003906 and Val loss: 1062756.25\n",
            "Train loss: 2487.7728576660156 and Val loss: 2115880.375\n",
            "Train loss: 4553.687408447266 and Val loss: 3168091.5\n",
            "Train loss: 6428.712432861328 and Val loss: 4219866.25\n",
            "Train loss: 7316.848480224609 and Val loss: 5261968.4375\n",
            "Train loss: 10328.456634521484 and Val loss: 6307214.9375\n",
            "Train loss: 12751.981048583984 and Val loss: 7355606.75\n",
            "Train loss: 14142.668914794922 and Val loss: 8393881.9375\n",
            "Train loss: 15890.940032958984 and Val loss: 9432945.125\n",
            "Train loss: 18087.37289428711 and Val loss: 10489022.5\n",
            "Train loss: 18551.06622314453 and Val loss: 11531961.1875\n",
            "Train loss: 21740.437072753906 and Val loss: 12585410.0625\n",
            "Train loss: 23139.71697998047 and Val loss: 13636242.3125\n",
            "Train loss: 1161.4893798828125 and Val loss: 1056666.75\n",
            "Train loss: 4253.4110107421875 and Val loss: 2114991.375\n",
            "Train loss: 6417.5333251953125 and Val loss: 3175354.375\n",
            "Train loss: 7895.609130859375 and Val loss: 4234435.125\n",
            "Train loss: 9192.871215820312 and Val loss: 5296096.875\n",
            "Train loss: 12086.204956054688 and Val loss: 6348865.375\n",
            "Train loss: 15048.508422851562 and Val loss: 7411269.25\n",
            "Train loss: 16065.923461914062 and Val loss: 8467847.875\n",
            "Train loss: 19380.464965820312 and Val loss: 9528500.0\n",
            "Train loss: 20295.850219726562 and Val loss: 10596608.25\n",
            "Train loss: 21546.546875 and Val loss: 11660366.375\n",
            "Train loss: 22834.609619140625 and Val loss: 12731433.25\n",
            "Train loss: 24880.960815429688 and Val loss: 13789792.875\n",
            "Train loss: 1513.9508056640625 and Val loss: 1038911.375\n",
            "Train loss: 5591.1270751953125 and Val loss: 2078522.9375\n",
            "Train loss: 7295.86083984375 and Val loss: 3121957.4375\n",
            "Train loss: 8491.063232421875 and Val loss: 4176186.8125\n",
            "Train loss: 9449.959533691406 and Val loss: 5214895.0625\n",
            "Train loss: 10030.475952148438 and Val loss: 6268056.3125\n",
            "Train loss: 11161.5869140625 and Val loss: 7312859.6875\n",
            "Train loss: 13223.634521484375 and Val loss: 8361386.0\n",
            "Train loss: 16367.659423828125 and Val loss: 9419277.25\n",
            "Train loss: 21112.395263671875 and Val loss: 10471374.625\n",
            "Train loss: 25743.334228515625 and Val loss: 11522018.625\n",
            "Train loss: 26952.868286132812 and Val loss: 12575741.0\n",
            "Train loss: 28490.695068359375 and Val loss: 13633453.25\n",
            "Train loss: 1197.010009765625 and Val loss: 1062842.625\n",
            "Train loss: 2281.9727783203125 and Val loss: 2129909.0\n",
            "Train loss: 3073.8453369140625 and Val loss: 3199894.5\n",
            "Train loss: 4651.5731201171875 and Val loss: 4263597.5\n",
            "Train loss: 6032.680419921875 and Val loss: 5329986.75\n",
            "Train loss: 7798.9952392578125 and Val loss: 6389897.25\n",
            "Train loss: 9124.02734375 and Val loss: 7462093.0\n",
            "Train loss: 9593.482147216797 and Val loss: 8528300.125\n",
            "Train loss: 9977.038635253906 and Val loss: 9597757.75\n",
            "Train loss: 11771.569763183594 and Val loss: 10670835.625\n",
            "Train loss: 12606.009155273438 and Val loss: 11748930.625\n",
            "Train loss: 14459.138061523438 and Val loss: 12821752.125\n",
            "Train loss: 15932.871826171875 and Val loss: 13893298.875\n",
            "Train loss: 1866.8582763671875 and Val loss: 1075937.25\n",
            "Train loss: 3628.27783203125 and Val loss: 2141558.5\n",
            "Train loss: 4489.51806640625 and Val loss: 3214149.625\n",
            "Train loss: 5112.0078125 and Val loss: 4282346.625\n",
            "Train loss: 7057.003173828125 and Val loss: 5360937.125\n",
            "Train loss: 8754.627197265625 and Val loss: 6436104.0\n",
            "Train loss: 9464.101623535156 and Val loss: 7512526.0\n",
            "Train loss: 10590.205627441406 and Val loss: 8579695.75\n",
            "Train loss: 11608.996643066406 and Val loss: 9648907.375\n",
            "Train loss: 12513.0703125 and Val loss: 10708427.25\n",
            "Train loss: 13449.763671875 and Val loss: 11789316.0\n",
            "Train loss: 14427.482238769531 and Val loss: 12868977.5\n",
            "Train loss: 15546.172058105469 and Val loss: 13932239.75\n",
            "Train loss: 992.1492309570312 and Val loss: 1041291.5625\n",
            "Train loss: 2052.6421508789062 and Val loss: 2096767.9375\n",
            "Train loss: 8428.315979003906 and Val loss: 3143233.625\n",
            "Train loss: 9993.622741699219 and Val loss: 4183231.875\n",
            "Train loss: 11142.186828613281 and Val loss: 5205316.4375\n",
            "Train loss: 11940.857360839844 and Val loss: 6247311.75\n",
            "Train loss: 14042.419372558594 and Val loss: 7287415.3125\n",
            "Train loss: 16269.223327636719 and Val loss: 8337299.3125\n",
            "Train loss: 17909.882873535156 and Val loss: 9382040.3125\n",
            "Train loss: 19243.691833496094 and Val loss: 10441815.8125\n",
            "Train loss: 20033.416076660156 and Val loss: 11500782.0625\n",
            "Train loss: 21406.349548339844 and Val loss: 12540011.5625\n",
            "Train loss: 24026.63934326172 and Val loss: 13583611.0625\n",
            "Train loss: 774.1424560546875 and Val loss: 1067877.75\n",
            "Train loss: 2784.74853515625 and Val loss: 2141763.25\n",
            "Train loss: 4513.5628662109375 and Val loss: 3223450.625\n",
            "Train loss: 6214.1148681640625 and Val loss: 4301734.5\n",
            "Train loss: 6675.836029052734 and Val loss: 5374867.25\n",
            "Train loss: 7470.678009033203 and Val loss: 6448150.375\n",
            "Train loss: 8841.23452758789 and Val loss: 7519140.25\n",
            "Train loss: 10394.022491455078 and Val loss: 8597426.125\n",
            "Train loss: 11213.84146118164 and Val loss: 9670637.375\n",
            "Train loss: 12286.628448486328 and Val loss: 10737113.875\n",
            "Train loss: 14821.824981689453 and Val loss: 11803982.125\n",
            "Train loss: 19164.185821533203 and Val loss: 12869711.25\n",
            "Train loss: 20684.251495361328 and Val loss: 13934242.25\n",
            "Train loss: 662.5771484375 and Val loss: 1067905.5\n",
            "Train loss: 1530.4688720703125 and Val loss: 2141250.375\n",
            "Train loss: 2649.3486328125 and Val loss: 3202885.625\n",
            "Train loss: 3792.888671875 and Val loss: 4273691.75\n",
            "Train loss: 5516.477783203125 and Val loss: 5341006.0\n",
            "Train loss: 6109.606689453125 and Val loss: 6406389.125\n",
            "Train loss: 6617.644439697266 and Val loss: 7479587.5\n",
            "Train loss: 7386.900726318359 and Val loss: 8543970.75\n",
            "Train loss: 8984.66390991211 and Val loss: 9606934.125\n",
            "Train loss: 11273.35824584961 and Val loss: 10671852.125\n",
            "Train loss: 12230.57357788086 and Val loss: 11734679.375\n",
            "Train loss: 12840.387237548828 and Val loss: 12796683.125\n",
            "Train loss: 14841.54702758789 and Val loss: 13858285.25\n",
            "Train loss: 969.5678100585938 and Val loss: 1045513.5\n",
            "Train loss: 1682.0374755859375 and Val loss: 2092257.0625\n",
            "Train loss: 3078.189697265625 and Val loss: 3133949.125\n",
            "Train loss: 4295.139892578125 and Val loss: 4181729.625\n",
            "Train loss: 5547.76904296875 and Val loss: 5225316.375\n",
            "Train loss: 9093.244384765625 and Val loss: 6269760.25\n",
            "Train loss: 9752.286010742188 and Val loss: 7312156.4375\n",
            "Train loss: 11392.264526367188 and Val loss: 8359156.6875\n",
            "Train loss: 12431.638427734375 and Val loss: 9411482.1875\n",
            "Train loss: 14734.73681640625 and Val loss: 10456389.1875\n",
            "Train loss: 15659.859375 and Val loss: 11511357.1875\n",
            "Train loss: 16832.274291992188 and Val loss: 12549489.125\n",
            "Train loss: 19241.711791992188 and Val loss: 13584320.8125\n",
            "Train loss: 1112.1766357421875 and Val loss: 1035461.6875\n",
            "Train loss: 1990.3763427734375 and Val loss: 2072981.625\n",
            "Train loss: 2574.3362426757812 and Val loss: 3101242.75\n",
            "Train loss: 3121.1344604492188 and Val loss: 4144792.3125\n",
            "Train loss: 4831.764343261719 and Val loss: 5179252.625\n",
            "Train loss: 6428.666442871094 and Val loss: 6225350.6875\n",
            "Train loss: 7072.2449951171875 and Val loss: 7262621.8125\n",
            "Train loss: 8331.994506835938 and Val loss: 8322068.0625\n",
            "Train loss: 9297.527893066406 and Val loss: 9366885.25\n",
            "Train loss: 10276.141967773438 and Val loss: 10408860.6875\n",
            "Train loss: 14018.443237304688 and Val loss: 11459758.6875\n",
            "Train loss: 14727.500915527344 and Val loss: 12504242.5625\n",
            "Train loss: 16567.37225341797 and Val loss: 13524876.9375\n",
            "Train loss: 5347.06787109375 and Val loss: 1073077.625\n",
            "Train loss: 8046.533935546875 and Val loss: 2151733.0\n",
            "Train loss: 10759.663330078125 and Val loss: 3233567.75\n",
            "Train loss: 14037.737548828125 and Val loss: 4304762.0\n",
            "Train loss: 17767.491455078125 and Val loss: 5377191.0\n",
            "Train loss: 19721.150634765625 and Val loss: 6439942.5\n",
            "Train loss: 21022.39208984375 and Val loss: 7487357.5625\n",
            "Train loss: 23783.090087890625 and Val loss: 8534499.5625\n",
            "Train loss: 24465.21405029297 and Val loss: 9574174.1875\n",
            "Train loss: 27981.382751464844 and Val loss: 10595142.75\n",
            "Train loss: 28907.793334960938 and Val loss: 11621326.5625\n",
            "Train loss: 34526.28063964844 and Val loss: 12672230.1875\n",
            "Train loss: 35753.92712402344 and Val loss: 13725682.4375\n",
            "Train loss: 1136.9808349609375 and Val loss: 1051968.375\n",
            "Train loss: 1999.55078125 and Val loss: 2107273.25\n",
            "Train loss: 2833.1619262695312 and Val loss: 3148526.25\n",
            "Train loss: 4922.423400878906 and Val loss: 4207088.375\n",
            "Train loss: 7773.046691894531 and Val loss: 5268159.25\n",
            "Train loss: 9914.626770019531 and Val loss: 6335577.0\n",
            "Train loss: 11647.793640136719 and Val loss: 7402682.5\n",
            "Train loss: 13334.939514160156 and Val loss: 8455329.625\n",
            "Train loss: 14380.495788574219 and Val loss: 9517698.125\n",
            "Train loss: 16222.036437988281 and Val loss: 10556567.4375\n",
            "Train loss: 19036.95684814453 and Val loss: 11598727.5625\n",
            "Train loss: 25319.95440673828 and Val loss: 12654094.9375\n",
            "Train loss: 27526.760314941406 and Val loss: 13705410.9375\n",
            "Train loss: 2071.91064453125 and Val loss: 1060811.625\n",
            "Train loss: 4266.276611328125 and Val loss: 2115536.625\n",
            "Train loss: 5532.6505126953125 and Val loss: 3179184.5\n",
            "Train loss: 6940.53369140625 and Val loss: 4218617.8125\n",
            "Train loss: 10344.728271484375 and Val loss: 5279907.8125\n",
            "Train loss: 12847.084716796875 and Val loss: 6341357.3125\n",
            "Train loss: 13754.779663085938 and Val loss: 7402480.4375\n",
            "Train loss: 15305.458984375 and Val loss: 8465239.3125\n",
            "Train loss: 16732.686401367188 and Val loss: 9526803.1875\n",
            "Train loss: 19758.471313476562 and Val loss: 10585515.1875\n",
            "Train loss: 21541.802978515625 and Val loss: 11643699.5625\n",
            "Train loss: 22843.14453125 and Val loss: 12706006.9375\n",
            "Train loss: 23675.46484375 and Val loss: 13771653.3125\n",
            "Train loss: 1333.3055419921875 and Val loss: 1074839.375\n",
            "Train loss: 2324.5602416992188 and Val loss: 2150635.125\n",
            "Train loss: 4778.997985839844 and Val loss: 3225951.5\n",
            "Train loss: 5675.8731689453125 and Val loss: 4298558.625\n",
            "Train loss: 7077.828369140625 and Val loss: 5362073.5\n",
            "Train loss: 8230.8203125 and Val loss: 6436994.875\n",
            "Train loss: 8728.082122802734 and Val loss: 7503926.875\n",
            "Train loss: 9995.594451904297 and Val loss: 8567568.75\n",
            "Train loss: 12368.512176513672 and Val loss: 9634105.875\n",
            "Train loss: 13197.901763916016 and Val loss: 10700813.375\n",
            "Train loss: 14140.639251708984 and Val loss: 11762498.75\n",
            "Train loss: 15091.774627685547 and Val loss: 12829846.25\n",
            "Train loss: 15908.87124633789 and Val loss: 13901418.25\n",
            "Train loss: 3676.935302734375 and Val loss: 1004779.5\n",
            "Train loss: 5339.4752197265625 and Val loss: 2013469.75\n",
            "Train loss: 12377.281372070312 and Val loss: 3037078.1875\n",
            "Train loss: 14859.314575195312 and Val loss: 4062820.625\n",
            "Train loss: 16196.031005859375 and Val loss: 5077116.5625\n",
            "Train loss: 28347.072998046875 and Val loss: 6091432.4375\n",
            "Train loss: 34373.787353515625 and Val loss: 7101991.5625\n",
            "Train loss: 35919.70520019531 and Val loss: 8137137.5\n",
            "Train loss: 37242.33239746094 and Val loss: 9163454.0625\n",
            "Train loss: 38749.165283203125 and Val loss: 10178702.4375\n",
            "Train loss: 43900.809814453125 and Val loss: 11199706.375\n",
            "Train loss: 48063.503662109375 and Val loss: 12218796.5625\n",
            "Train loss: 51992.01611328125 and Val loss: 13245561.3125\n",
            "Train loss: 2143.154052734375 and Val loss: 1079199.25\n",
            "Train loss: 4217.00341796875 and Val loss: 2165698.125\n",
            "Train loss: 6418.7666015625 and Val loss: 3240132.0\n",
            "Train loss: 10056.76318359375 and Val loss: 4318968.625\n",
            "Train loss: 13320.70263671875 and Val loss: 5394121.125\n",
            "Train loss: 17191.038330078125 and Val loss: 6460194.625\n",
            "Train loss: 18378.666381835938 and Val loss: 7547232.375\n",
            "Train loss: 19608.653564453125 and Val loss: 8604367.0\n",
            "Train loss: 20730.874633789062 and Val loss: 9662808.375\n",
            "Train loss: 22514.036376953125 and Val loss: 10717469.25\n",
            "Train loss: 28536.554931640625 and Val loss: 11793511.125\n",
            "Train loss: 30205.346557617188 and Val loss: 12864602.375\n",
            "Train loss: 31257.977661132812 and Val loss: 13936714.375\n",
            "Train loss: 3029.764404296875 and Val loss: 1065309.75\n",
            "Train loss: 3793.7114868164062 and Val loss: 2125388.5\n",
            "Train loss: 5469.538879394531 and Val loss: 3175512.0\n",
            "Train loss: 7494.450866699219 and Val loss: 4242010.875\n",
            "Train loss: 8859.429992675781 and Val loss: 5289792.75\n",
            "Train loss: 10674.139831542969 and Val loss: 6356940.625\n",
            "Train loss: 11980.753356933594 and Val loss: 7422502.25\n",
            "Train loss: 14074.715515136719 and Val loss: 8495030.375\n",
            "Train loss: 16445.76678466797 and Val loss: 9564049.25\n",
            "Train loss: 17367.203247070312 and Val loss: 10624612.625\n",
            "Train loss: 19470.713012695312 and Val loss: 11694466.625\n",
            "Train loss: 20713.512817382812 and Val loss: 12758344.875\n",
            "Train loss: 22543.666015625 and Val loss: 13816857.625\n",
            "Train loss: 2381.462158203125 and Val loss: 1049850.75\n",
            "Train loss: 3484.5626220703125 and Val loss: 2104279.5\n",
            "Train loss: 5521.1732177734375 and Val loss: 3167251.75\n",
            "Train loss: 6665.810791015625 and Val loss: 4207519.5625\n",
            "Train loss: 8653.150634765625 and Val loss: 5247672.375\n",
            "Train loss: 11069.951416015625 and Val loss: 6305473.25\n",
            "Train loss: 12015.583190917969 and Val loss: 7374148.625\n",
            "Train loss: 12933.2958984375 and Val loss: 8427968.375\n",
            "Train loss: 14115.833740234375 and Val loss: 9479617.125\n",
            "Train loss: 15253.190795898438 and Val loss: 10536687.25\n",
            "Train loss: 16818.285888671875 and Val loss: 11602579.0\n",
            "Train loss: 18853.3408203125 and Val loss: 12664705.0\n",
            "Train loss: 20007.148803710938 and Val loss: 13720510.0\n",
            "Train loss: 2494.123779296875 and Val loss: 1047402.625\n",
            "Train loss: 3859.404052734375 and Val loss: 2084789.9375\n",
            "Train loss: 4823.2161865234375 and Val loss: 3125659.875\n",
            "Train loss: 5315.973327636719 and Val loss: 4171662.5\n",
            "Train loss: 6194.866027832031 and Val loss: 5212921.75\n",
            "Train loss: 7794.244323730469 and Val loss: 6255652.0625\n",
            "Train loss: 11545.291198730469 and Val loss: 7312536.9375\n",
            "Train loss: 12573.723205566406 and Val loss: 8335422.125\n",
            "Train loss: 13877.544372558594 and Val loss: 9385936.125\n",
            "Train loss: 14688.981567382812 and Val loss: 10434580.5\n",
            "Train loss: 15542.702087402344 and Val loss: 11478752.3125\n",
            "Train loss: 18183.52655029297 and Val loss: 12520702.6875\n",
            "Train loss: 18963.390014648438 and Val loss: 13560285.0\n",
            "Train loss: 647.1388549804688 and Val loss: 1048856.375\n",
            "Train loss: 3054.8744506835938 and Val loss: 2103685.375\n",
            "Train loss: 3540.3504333496094 and Val loss: 3136322.625\n",
            "Train loss: 5598.778167724609 and Val loss: 4192594.875\n",
            "Train loss: 6964.365570068359 and Val loss: 5239326.375\n",
            "Train loss: 7752.730377197266 and Val loss: 6286164.875\n",
            "Train loss: 13974.657135009766 and Val loss: 7328460.125\n",
            "Train loss: 16060.878326416016 and Val loss: 8373107.625\n",
            "Train loss: 19266.12124633789 and Val loss: 9419307.0625\n",
            "Train loss: 20846.35171508789 and Val loss: 10462779.5\n",
            "Train loss: 22494.075958251953 and Val loss: 11514759.625\n",
            "Train loss: 24704.029571533203 and Val loss: 12558151.9375\n",
            "Train loss: 25471.212615966797 and Val loss: 13604910.1875\n",
            "Train loss: 1256.533203125 and Val loss: 1034645.1875\n",
            "Train loss: 2135.2172241210938 and Val loss: 2077854.9375\n",
            "Train loss: 3114.6581420898438 and Val loss: 3131665.1875\n",
            "Train loss: 4352.176452636719 and Val loss: 4175990.9375\n",
            "Train loss: 5213.904602050781 and Val loss: 5222466.5625\n",
            "Train loss: 5962.982177734375 and Val loss: 6267394.3125\n",
            "Train loss: 7262.1842041015625 and Val loss: 7312038.0625\n",
            "Train loss: 8421.498657226562 and Val loss: 8360635.6875\n",
            "Train loss: 9142.823120117188 and Val loss: 9409658.9375\n",
            "Train loss: 10466.739990234375 and Val loss: 10463188.5625\n",
            "Train loss: 11696.89794921875 and Val loss: 11511688.0625\n",
            "Train loss: 12910.333374023438 and Val loss: 12559799.0\n",
            "Train loss: 13839.057983398438 and Val loss: 13613909.125\n",
            "Train loss: 1315.01953125 and Val loss: 1032170.8125\n",
            "Train loss: 2307.0714111328125 and Val loss: 2063597.625\n",
            "Train loss: 3464.903076171875 and Val loss: 3099714.75\n",
            "Train loss: 4080.3460693359375 and Val loss: 4116930.5\n",
            "Train loss: 5933.366455078125 and Val loss: 5149686.6875\n",
            "Train loss: 8610.9921875 and Val loss: 6189458.5\n",
            "Train loss: 11176.489501953125 and Val loss: 7232511.375\n",
            "Train loss: 12548.170776367188 and Val loss: 8275335.0\n",
            "Train loss: 14167.967651367188 and Val loss: 9316586.875\n",
            "Train loss: 15709.651245117188 and Val loss: 10356861.625\n",
            "Train loss: 17019.856811523438 and Val loss: 11405570.625\n",
            "Train loss: 18078.572875976562 and Val loss: 12423975.25\n",
            "Train loss: 20570.127319335938 and Val loss: 13443190.9375\n",
            "Train loss: 1494.6187744140625 and Val loss: 1071430.25\n",
            "Train loss: 2616.6053466796875 and Val loss: 2140715.875\n",
            "Train loss: 3358.2950439453125 and Val loss: 3213573.125\n",
            "Train loss: 4742.11865234375 and Val loss: 4278923.0\n",
            "Train loss: 5868.4483642578125 and Val loss: 5353560.75\n",
            "Train loss: 10203.557739257812 and Val loss: 6427396.125\n",
            "Train loss: 11249.026733398438 and Val loss: 7505801.125\n",
            "Train loss: 12429.929321289062 and Val loss: 8556874.5\n",
            "Train loss: 13853.567504882812 and Val loss: 9628025.375\n",
            "Train loss: 14807.094665527344 and Val loss: 10696902.625\n",
            "Train loss: 15712.208374023438 and Val loss: 11774995.5\n",
            "Train loss: 16644.4921875 and Val loss: 12840531.5\n",
            "Train loss: 17964.1552734375 and Val loss: 13891207.25\n",
            "Train loss: 932.619140625 and Val loss: 1030296.8125\n",
            "Train loss: 2145.6971435546875 and Val loss: 2067900.125\n",
            "Train loss: 3765.6456298828125 and Val loss: 3105885.5625\n",
            "Train loss: 5024.171875 and Val loss: 4146099.8125\n",
            "Train loss: 6822.339111328125 and Val loss: 5174876.625\n",
            "Train loss: 7870.3155517578125 and Val loss: 6218184.8125\n",
            "Train loss: 8541.921691894531 and Val loss: 7257441.625\n",
            "Train loss: 9898.980773925781 and Val loss: 8301117.6875\n",
            "Train loss: 10764.439392089844 and Val loss: 9342884.4375\n",
            "Train loss: 11526.286010742188 and Val loss: 10394470.5625\n",
            "Train loss: 12499.947570800781 and Val loss: 11437552.9375\n",
            "Train loss: 13615.768249511719 and Val loss: 12474907.1875\n",
            "Train loss: 16496.419372558594 and Val loss: 13501418.0625\n",
            "Train loss: 1109.4156494140625 and Val loss: 1100278.5\n",
            "Train loss: 1634.8456420898438 and Val loss: 2168584.125\n",
            "Train loss: 2593.3081665039062 and Val loss: 3251836.375\n",
            "Train loss: 5162.266418457031 and Val loss: 4316607.375\n",
            "Train loss: 7893.422424316406 and Val loss: 5373321.25\n",
            "Train loss: 9087.696105957031 and Val loss: 6441022.0\n",
            "Train loss: 10720.441711425781 and Val loss: 7507160.625\n",
            "Train loss: 11529.840393066406 and Val loss: 8571159.375\n",
            "Train loss: 13596.089416503906 and Val loss: 9638399.875\n",
            "Train loss: 15384.171813964844 and Val loss: 10712432.25\n",
            "Train loss: 18370.83856201172 and Val loss: 11789456.25\n",
            "Train loss: 23203.32440185547 and Val loss: 12873458.0\n",
            "Train loss: 29423.59783935547 and Val loss: 13965895.25\n",
            "Train loss: 1241.767578125 and Val loss: 1064514.625\n",
            "Train loss: 1764.9557495117188 and Val loss: 2131729.5\n",
            "Train loss: 3317.3208618164062 and Val loss: 3198516.0\n",
            "Train loss: 4161.777038574219 and Val loss: 4276354.25\n",
            "Train loss: 5630.272155761719 and Val loss: 5354400.25\n",
            "Train loss: 7117.469299316406 and Val loss: 6428579.875\n",
            "Train loss: 8837.084411621094 and Val loss: 7495119.25\n",
            "Train loss: 10073.406311035156 and Val loss: 8570193.875\n",
            "Train loss: 10904.137451171875 and Val loss: 9641299.875\n",
            "Train loss: 12166.051391601562 and Val loss: 10715585.625\n",
            "Train loss: 13192.114501953125 and Val loss: 11784773.625\n",
            "Train loss: 13646.74154663086 and Val loss: 12850362.875\n",
            "Train loss: 16164.425872802734 and Val loss: 13923444.0\n",
            "Train loss: 1052.262939453125 and Val loss: 1061935.0\n",
            "Train loss: 3121.234375 and Val loss: 2115609.0\n",
            "Train loss: 4241.913818359375 and Val loss: 3174118.875\n",
            "Train loss: 5718.6883544921875 and Val loss: 4232314.25\n",
            "Train loss: 8165.9149169921875 and Val loss: 5290449.625\n",
            "Train loss: 9946.146240234375 and Val loss: 6352951.625\n",
            "Train loss: 12982.144287109375 and Val loss: 7417738.125\n",
            "Train loss: 15285.61962890625 and Val loss: 8482764.875\n",
            "Train loss: 15743.550018310547 and Val loss: 9541478.25\n",
            "Train loss: 17620.60238647461 and Val loss: 10581398.125\n",
            "Train loss: 24623.81820678711 and Val loss: 11633124.5\n",
            "Train loss: 26331.515106201172 and Val loss: 12703955.75\n",
            "Train loss: 29152.310760498047 and Val loss: 13768747.625\n",
            "Train loss: 793.9501342773438 and Val loss: 1030590.5\n",
            "Train loss: 1495.8216552734375 and Val loss: 2062698.3125\n",
            "Train loss: 3324.477783203125 and Val loss: 3097847.875\n",
            "Train loss: 5261.003173828125 and Val loss: 4116164.8125\n",
            "Train loss: 6487.47900390625 and Val loss: 5161381.0625\n",
            "Train loss: 8061.017822265625 and Val loss: 6182551.125\n",
            "Train loss: 10845.111328125 and Val loss: 7216293.0\n",
            "Train loss: 12001.927368164062 and Val loss: 8256320.875\n",
            "Train loss: 13055.9248046875 and Val loss: 9292932.3125\n",
            "Train loss: 14020.730651855469 and Val loss: 10331777.0625\n",
            "Train loss: 14793.299682617188 and Val loss: 11383563.4375\n",
            "Train loss: 15731.900085449219 and Val loss: 12413644.625\n",
            "Train loss: 22806.69696044922 and Val loss: 13464421.125\n",
            "Train loss: 835.2758178710938 and Val loss: 975861.0625\n",
            "Train loss: 1974.2881469726562 and Val loss: 1948750.4375\n",
            "Train loss: 4541.778137207031 and Val loss: 2943235.5625\n",
            "Train loss: 5265.941345214844 and Val loss: 3955547.125\n",
            "Train loss: 7345.206970214844 and Val loss: 4957209.5\n",
            "Train loss: 8156.718933105469 and Val loss: 5975630.0\n",
            "Train loss: 10267.350769042969 and Val loss: 7019171.125\n",
            "Train loss: 11600.544616699219 and Val loss: 8039131.5625\n",
            "Train loss: 14012.477478027344 and Val loss: 9052750.1875\n",
            "Train loss: 15086.009094238281 and Val loss: 10090328.375\n",
            "Train loss: 17710.71563720703 and Val loss: 11128797.1875\n",
            "Train loss: 21084.617736816406 and Val loss: 12157187.1875\n",
            "Train loss: 25086.983459472656 and Val loss: 13188230.1875\n",
            "Train loss: 2785.74462890625 and Val loss: 1088662.125\n",
            "Train loss: 4609.9544677734375 and Val loss: 2166216.0\n",
            "Train loss: 5843.656494140625 and Val loss: 3248924.5\n",
            "Train loss: 7175.4285888671875 and Val loss: 4322725.375\n",
            "Train loss: 8033.927917480469 and Val loss: 5409670.25\n",
            "Train loss: 9135.730163574219 and Val loss: 6495608.125\n",
            "Train loss: 10016.688781738281 and Val loss: 7583283.875\n",
            "Train loss: 11276.508728027344 and Val loss: 8673492.125\n",
            "Train loss: 12084.880981445312 and Val loss: 9759299.875\n",
            "Train loss: 13083.538818359375 and Val loss: 10844350.875\n",
            "Train loss: 13726.849853515625 and Val loss: 11923740.25\n",
            "Train loss: 14490.262756347656 and Val loss: 12993639.625\n",
            "Train loss: 17014.815490722656 and Val loss: 14065852.0\n",
            "Train loss: 617.5084228515625 and Val loss: 1093208.625\n",
            "Train loss: 3532.2947998046875 and Val loss: 2185345.75\n",
            "Train loss: 5389.9346923828125 and Val loss: 3269654.25\n",
            "Train loss: 6977.8206787109375 and Val loss: 4350933.375\n",
            "Train loss: 7775.279846191406 and Val loss: 5442266.25\n",
            "Train loss: 10232.104309082031 and Val loss: 6533459.375\n",
            "Train loss: 12850.710998535156 and Val loss: 7618743.125\n",
            "Train loss: 13306.900207519531 and Val loss: 8702956.25\n",
            "Train loss: 15126.821350097656 and Val loss: 9799061.625\n",
            "Train loss: 17033.621520996094 and Val loss: 10886802.0\n",
            "Train loss: 17673.310424804688 and Val loss: 11980342.25\n",
            "Train loss: 18640.648315429688 and Val loss: 13071383.875\n",
            "Train loss: 20026.726318359375 and Val loss: 14152801.875\n",
            "Train loss: 2182.135986328125 and Val loss: 1072158.875\n",
            "Train loss: 3665.065673828125 and Val loss: 2152242.875\n",
            "Train loss: 5191.6644287109375 and Val loss: 3226464.625\n",
            "Train loss: 6040.841796875 and Val loss: 4304777.625\n",
            "Train loss: 11141.607421875 and Val loss: 5403874.375\n",
            "Train loss: 11778.990539550781 and Val loss: 6493463.75\n",
            "Train loss: 12510.709350585938 and Val loss: 7578431.875\n",
            "Train loss: 13737.84814453125 and Val loss: 8691443.125\n",
            "Train loss: 15435.082397460938 and Val loss: 9794317.875\n",
            "Train loss: 16940.159790039062 and Val loss: 10886153.0\n",
            "Train loss: 18099.119384765625 and Val loss: 11969002.0\n",
            "Train loss: 21627.125 and Val loss: 13058506.375\n",
            "Train loss: 22508.6171875 and Val loss: 14137273.125\n",
            "Train loss: 626.8458862304688 and Val loss: 1064843.375\n",
            "Train loss: 3060.5348510742188 and Val loss: 2113764.375\n",
            "Train loss: 3711.4176025390625 and Val loss: 3157206.6875\n",
            "Train loss: 4602.563781738281 and Val loss: 4192486.375\n",
            "Train loss: 5302.961181640625 and Val loss: 5220608.0\n",
            "Train loss: 6996.870361328125 and Val loss: 6259797.125\n",
            "Train loss: 8017.986755371094 and Val loss: 7305847.75\n",
            "Train loss: 9405.407409667969 and Val loss: 8349462.0625\n",
            "Train loss: 10589.765441894531 and Val loss: 9409436.5625\n",
            "Train loss: 13929.997619628906 and Val loss: 10457029.3125\n",
            "Train loss: 15724.546081542969 and Val loss: 11508449.4375\n",
            "Train loss: 18242.939392089844 and Val loss: 12568769.0625\n",
            "Train loss: 18944.558715820312 and Val loss: 13613715.0\n",
            "Train loss: 2517.658447265625 and Val loss: 1001189.3125\n",
            "Train loss: 4145.6158447265625 and Val loss: 1990544.5625\n",
            "Train loss: 6540.0977783203125 and Val loss: 3019634.375\n",
            "Train loss: 9601.830932617188 and Val loss: 4039084.25\n",
            "Train loss: 12175.229370117188 and Val loss: 5056083.625\n",
            "Train loss: 13063.770812988281 and Val loss: 6081795.0625\n",
            "Train loss: 16795.537170410156 and Val loss: 7136376.4375\n",
            "Train loss: 17955.714416503906 and Val loss: 8207409.3125\n",
            "Train loss: 19985.661682128906 and Val loss: 9267765.1875\n",
            "Train loss: 21309.67987060547 and Val loss: 10333327.6875\n",
            "Train loss: 23045.49395751953 and Val loss: 11388856.0625\n",
            "Train loss: 27446.74786376953 and Val loss: 12433546.6875\n",
            "Train loss: 30618.241271972656 and Val loss: 13503629.9375\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}